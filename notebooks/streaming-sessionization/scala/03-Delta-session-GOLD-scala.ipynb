{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78448b0d-e386-4dba-83d2-bbba4e34acdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png)  3/ GOLD table: extract the sessions\n",
    "\n",
    "<img style=\"float:right; height: 250px; margin: 0px 30px 0px 30px\" src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/main/images/product/streaming-sessionization/session_diagram.png\">\n",
    "\n",
    "**Scala version:** This notebook implement the same logic as [the python]($../03-Delta-session-GOLD), but using Scala. As you'll see, the function signature is slightly different as we do not receive an iterator of Pandas Dataframe, but the logic remains identical.\n",
    "\n",
    "### Why is this a challenge?\n",
    "Because we don't have any event to flag the user disconnection, detecting the end of the session is hard. After 10 minutes without any events, we want to be notified that the session has ended.\n",
    "However, spark will only react on event, not the absence of event.\n",
    "\n",
    "Thanksfully, Spark Structured Streaming has the concept of timeout. \n",
    "\n",
    "**We can set a 10 minutes timeout in the state engine** and be notified 10 minutes later in order to close the session\n",
    "\n",
    "<!-- tracking, please Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-engineering&org_id=1126292079753158&notebook=%2Fscala%2F03-Delta-session-GOLD-scala&demo_name=streaming-sessionization&event=VIEW&path=%2F_dbdemos%2Fdata-engineering%2Fstreaming-sessionization%2Fscala%2F03-Delta-session-GOLD-scala&version=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b25f739-f753-43d0-9299-752d55a010cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": null
    }
   },
   "source": [
    "### A cluster has been created for this demo\n",
    "To run this demo, just select the cluster `dbdemos-streaming-sessionization-david_lopez` from the dropdown menu ([open cluster configuration](https://adb-1126292079753158.18.azuredatabricks.net/#setting/clusters/0102-204425-e6rayurl/configuration)). <br />\n",
    "*Note: If the cluster was deleted after 30 days, you can re-create it with `dbdemos.create_cluster('streaming-sessionization')` or re-install the demo: `dbdemos.install('streaming-sessionization')`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df5415b4-9725-4471-991c-978c6d9bd521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-setup-scala $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4081e8a7-557d-4078-b4cc-bbef53507ce4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Implementing the aggregation function to update our Session\n",
    "\n",
    "In this simple example, we'll just be counting the number of click in the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e7dd2da-6f88-4c71-bb3e-1ff56c2617b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import java.sql.Timestamp\n",
    "\n",
    "waitForTable(\"events\") // Wait until the previous table is created to avoid error if all notebooks are started at once\n",
    "\n",
    "//Event (from the silver table)\n",
    "case class ClickEvent(user_id: String, event_id: String, event_datetime: Timestamp, event_date: Long, platform: String, action: String, uri: String)\n",
    "//Session (from the gold table)\n",
    "case class UserSession(\n",
    "  user_id: String, \n",
    "  var click_count: Integer = 0, \n",
    "  var start_time: Timestamp = Timestamp.valueOf(\"9999-12-31 23:59:29\"), \n",
    "  var end_time: Timestamp = new Timestamp(0L), \n",
    "  var status: String = \"online\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39ab5d13-2ad3-4692-98f6-09868fe8eb68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The function `updateState` will be called for each user with a list of events for this user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd444bd4-bebd-40e9-8de7-a6a5bc8b2d55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.streaming.{ GroupState, GroupStateTimeout, OutputMode }\n",
    "\n",
    "\n",
    "val MaxSessionDuration = 30000\n",
    "\n",
    "def updateState(user_id: String, events: Iterator[ClickEvent], state: GroupState[UserSession]): Iterator[UserSession] = {\n",
    "  val curState = state.getOption.getOrElse { UserSession(user_id) } // get previous state or instantiate new with default\n",
    "  if (state.hasTimedOut) {\n",
    "    state.remove()\n",
    "    Iterator(curState)\n",
    "  } else {\n",
    "    val updatedState = events.foldLeft(curState){ updateStateWithEvent }\n",
    "    updatedState.status = \"offline\" // next iteration will be a timeout or restart\n",
    "    state.update(updatedState)\n",
    "    state.setTimeoutTimestamp(MaxSessionDuration)\n",
    "    Iterator(updatedState)\n",
    "  }\n",
    "}\n",
    "\n",
    "def updateStateWithEvent(state: UserSession, input: ClickEvent): UserSession = {\n",
    "  state.status = \"online\"\n",
    "  state.click_count += 1\n",
    "  //Update then begining and end of our session\n",
    "  if (input.event_datetime.after(state.end_time)) {\n",
    "    state.end_time = input.event_datetime\n",
    "  }\n",
    "  if (input.event_datetime.before(state.start_time)) {\n",
    "    state.start_time = input.event_datetime\n",
    "  }\n",
    "  state\n",
    "}\n",
    "\n",
    "val sessions = spark\n",
    "  .readStream\n",
    "  .format(\"delta\")\n",
    "  .table(\"events\")  \n",
    "  .as[ClickEvent]\n",
    "  .groupByKey(_.user_id)\n",
    "  .flatMapGroupsWithState(OutputMode.Append(), GroupStateTimeout.EventTimeTimeout)(updateState)\n",
    "  .toDF\n",
    "\n",
    "display(sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41355b5d-02c1-40db-b577-cd5b746cc492",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Updating the session table with number of clicks and end/start time\n",
    "\n",
    "We want to have the session information in real time for each user. \n",
    "\n",
    "To do that, we'll create a Session table. Everytime we update the state, we'll UPSERT the session information:\n",
    "\n",
    "- if the session doesn't exist, we add it\n",
    "- if it exists, we update it with the new count and potential new status\n",
    "\n",
    "This can easily be done with a MERGE operation using Delta and calling `foreachBatch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b826c3cc-74b5-4346-858a-a4e745ca7f6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import io.delta.tables.DeltaTable\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "def updateSessions(df: DataFrame, epochId: Long): Unit = {\n",
    "  // Create the table if it doesn't exist (we need it to be able to perform the merge)\n",
    "  if (!spark.catalog.tableExists(\"sessions\")) {\n",
    "    df.limit(0).write.option(\"mergeSchema\", \"true\").mode(\"append\").saveAsTable(\"sessions\")\n",
    "  }\n",
    "\n",
    "  DeltaTable.forName(spark, \"sessions\").alias(\"s\")\n",
    "    .merge(source = df.alias(\"u\"), condition = \"s.user_id = u.user_id\")\n",
    "    .whenMatched().updateAll()\n",
    "    .whenNotMatched().insertAll()\n",
    "    .execute()\n",
    "}\n",
    "\n",
    "sessions\n",
    "  .writeStream\n",
    "  .option(\"checkpointLocation\", s\"$cloudStoragePath/checkpoints/sessions\")\n",
    "  .foreachBatch(updateSessions _)\n",
    "  .start()\n",
    "\n",
    "waitForTable(\"sessions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da1fbef9-0a7b-4c3e-8773-cc3f53d60240",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50aee3bd-73ab-4b85-9606-2945cc50af9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql SELECT CAST(avg(end_time - start_time) as INT) average_session_duration FROM sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88172f61-a95e-4a0a-93b2-c267358381a9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Stop all the streams "
    }
   },
   "outputs": [],
   "source": [
    "stopAllStreams(sleepTime=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b540379-af5b-4545-a214-2c6ef78d5696",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### We now have our sessions stream running!\n",
    "\n",
    "We can set the output of this streaming job to a SQL database or another queuing system.\n",
    "\n",
    "We'll be able to automatically detect cart abandonments in our website and send an email to our customers, our maybe just give them a call asking if they need some help! "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "scala",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03-Delta-session-GOLD-scala",
   "widgets": {}
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
