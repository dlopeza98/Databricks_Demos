{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f629b521-0323-48bb-b849-101ab322a5ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Technical Setup notebook. Hide this cell results\n",
    "Initialize dataset to the current user and cleanup data when reset_all_data is set to true\n",
    "\n",
    "Do not edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8176f4b8-54ae-46e7-b648-d7c8b02523f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"reset_all_data\", \"false\", [\"true\", \"false\"], \"Reset all data\")\n",
    "dbutils.widgets.text(\"min_dbr_version\", \"9.1\", \"Min required DBR version\")\n",
    "#Empty value will try default: dbdemos with a fallback to hive_metastore\n",
    "#Specifying a value will not have fallback and fail if the catalog can't be used/created\n",
    "dbutils.widgets.text(\"catalog\", \"\", \"Catalog\")\n",
    "#Empty value will be set to a database scoped to the current user using db_prefix\n",
    "dbutils.widgets.text(\"db\", \"\", \"Database\")\n",
    "#ignored if db is set (we force the databse to the given value in this case)\n",
    "dbutils.widgets.text(\"db_prefix\", \"retail\", \"Database prefix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91247bbc-af33-413b-b00a-460587728a93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "import pandas as pd\n",
    "import logging\n",
    "from pyspark.sql.functions import to_date, col, regexp_extract, rand, to_timestamp, initcap, sha1\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, input_file_name, col\n",
    "import pyspark.sql.functions as F\n",
    "import re\n",
    "import time\n",
    "\n",
    "\n",
    "# VERIFY DATABRICKS VERSION COMPATIBILITY ----------\n",
    "\n",
    "try:\n",
    "  min_required_version = dbutils.widgets.get(\"min_dbr_version\")\n",
    "except:\n",
    "  min_required_version = \"9.1\"\n",
    "\n",
    "version_tag = spark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\")\n",
    "version_search = re.search('^([0-9]*\\.[0-9]*)', version_tag)\n",
    "assert version_search, f\"The Databricks version can't be extracted from {version_tag}, shouldn't happen, please correct the regex\"\n",
    "current_version = float(version_search.group(1))\n",
    "assert float(current_version) >= float(min_required_version), f'The Databricks version of the cluster must be >= {min_required_version}. Current version detected: {current_version}'\n",
    "assert \"ml\" in version_tag.lower(), f\"The Databricks ML runtime must be used. Current version detected doesn't contain 'ml': {version_tag} \"\n",
    "\n",
    "\n",
    "#python Imports for ML...\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking.client import MlflowClient\n",
    "from hyperopt import fmin, hp, tpe, STATUS_OK, Trials\n",
    "from hyperopt.pyll.base import scope\n",
    "from hyperopt import SparkTrials\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import os\n",
    "import pandas as pd\n",
    "from hyperopt import space_eval\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#force the experiment to the field demos one. Required to launch as a batch\n",
    "def init_experiment_for_batch(demo_name, experiment_name):\n",
    "  #You can programatically get a PAT token with the following\n",
    "  pat_token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "  url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "  #current_user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')\n",
    "  import requests\n",
    "  xp_root_path = f\"/dbdemos/experiments/{demo_name}\"\n",
    "  requests.post(f\"{url}/api/2.0/workspace/mkdirs\", headers = {\"Accept\": \"application/json\", \"Authorization\": f\"Bearer {pat_token}\"}, json={ \"path\": xp_root_path})\n",
    "  xp = f\"{xp_root_path}/{experiment_name}\"\n",
    "  print(f\"Using common experiment under {xp}\")\n",
    "  mlflow.set_experiment(xp)\n",
    "  return mlflow.get_experiment_by_name(xp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3f5c7d1-40ea-431a-b5e1-5005c426ee7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_cloud_name():\n",
    "  return spark.conf.get(\"spark.databricks.clusterUsageTags.cloudProvider\").lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b9be674-1ee8-432f-b51a-4a9d9bdfacd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.cloudFiles.schemaInference.sampleSize.numFiles\", \"10\")\n",
    "\n",
    "current_user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')\n",
    "if current_user.rfind('@') > 0:\n",
    "  current_user_no_at = current_user[:current_user.rfind('@')]\n",
    "else:\n",
    "  current_user_no_at = current_user\n",
    "current_user_no_at = re.sub(r'\\W+', '_', current_user_no_at)\n",
    "\n",
    "db = dbutils.widgets.get(\"db\")\n",
    "db_prefix = dbutils.widgets.get(\"db_prefix\")\n",
    "if len(db) == 0:\n",
    "  dbName = db_prefix+\"_\"+current_user_no_at\n",
    "else:\n",
    "  dbName = db\n",
    "  \n",
    "cloud_storage_path = f\"/Users/{current_user}/demos/{db_prefix}\"\n",
    "reset_all = dbutils.widgets.get(\"reset_all_data\") == \"true\"\n",
    "\n",
    "#Try to use the UC catalog \"dbdemos\" when possible. IF not will fallback to hive_metastore\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "\n",
    "def use_and_create_db(catalog, dbName, cloud_storage_path = None):\n",
    "  print(f\"USE CATALOG `{catalog}`\")\n",
    "  spark.sql(f\"USE CATALOG `{catalog}`\")\n",
    "  if cloud_storage_path is None or catalog not in ['hive_metastore', 'spark_catalog']:\n",
    "    spark.sql(f\"\"\"create database if not exists `{dbName}` \"\"\")\n",
    "  else:\n",
    "    spark.sql(f\"\"\"create database if not exists `{dbName}` LOCATION '{cloud_storage_path}/tables' \"\"\")\n",
    "\n",
    "if reset_all:\n",
    "  spark.sql(f\"DROP DATABASE IF EXISTS `{dbName}` CASCADE\")\n",
    "  dbutils.fs.rm(cloud_storage_path, True)\n",
    "\n",
    "if catalog == \"spark_catalog\":\n",
    "  catalog = \"hive_metastore\"\n",
    "  \n",
    "#If the catalog is defined, we force it to the given value and throw exception if not.\n",
    "if len(catalog) > 0:\n",
    "  current_catalog = spark.sql(\"select current_catalog()\").collect()[0]['current_catalog()']\n",
    "  if current_catalog != catalog:\n",
    "    catalogs = [r['catalog'] for r in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "    if catalog not in catalogs and catalog not in ['hive_metastore', 'spark_catalog']:\n",
    "      spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "  use_and_create_db(catalog, dbName)\n",
    "else:\n",
    "  #otherwise we'll try to setup the catalog to DBDEMOS and create the database here. If we can't we'll fallback to legacy hive_metastore\n",
    "  print(\"Try to setup UC catalog\")\n",
    "  try:\n",
    "    catalogs = [r['catalog'] for r in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "    if len(catalogs) == 1 and catalogs[0] in ['hive_metastore', 'spark_catalog']:\n",
    "      print(f\"UC doesn't appear to be enabled, will fallback to hive_metastore (spark_catalog)\")\n",
    "      catalog = \"hive_metastore\"\n",
    "    else:\n",
    "      if \"dbdemos\" not in catalogs:\n",
    "        spark.sql(\"CREATE CATALOG IF NOT EXISTS dbdemos\")\n",
    "      catalog = \"dbdemos\"\n",
    "    use_and_create_db(catalog, dbName)\n",
    "  except Exception as e:\n",
    "    print(f\"error with catalog {e}, do you have permission or UC enabled? will fallback to hive_metastore\")\n",
    "    catalog = \"hive_metastore\"\n",
    "    use_and_create_db(catalog, dbName)\n",
    "\n",
    "print(f\"using cloud_storage_path {cloud_storage_path}\")\n",
    "print(f\"using catalog.database `{catalog}`.`{dbName}`\")\n",
    "\n",
    "#Add the catalog to cloud storage path as we could have 1 checkpoint location different per catalog\n",
    "if catalog not in ['hive_metastore', 'spark_catalog']:\n",
    "  cloud_storage_path+=\"_\"+catalog\n",
    "  try:\n",
    "    spark.sql(f\"GRANT CREATE, USAGE on DATABASE {catalog}.{dbName} TO `account users`\")\n",
    "    spark.sql(f\"ALTER SCHEMA {catalog}.{dbName} OWNER TO `account users`\")\n",
    "  except Exception as e:\n",
    "    print(\"Couldn't grant access to the schema to all users:\"+str(e))\n",
    "  \n",
    "#with parallel execution this can fail the time of the initialization. add a few retry to fix these issues\n",
    "for i in range(10):\n",
    "  try:\n",
    "    spark.sql(f\"\"\"USE `{catalog}`.`{dbName}`\"\"\")\n",
    "    break\n",
    "  except Exception as e:\n",
    "    time.sleep(1)\n",
    "    if i >= 9:\n",
    "      raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0f39745-707b-4c34-86ae-2f3b18f2f045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def display_slide(slide_id, slide_number):\n",
    "  displayHTML(f'''\n",
    "  <div style=\"width:1150px; margin:auto\">\n",
    "  <iframe\n",
    "    src=\"https://docs.google.com/presentation/d/{slide_id}/embed?slide={slide_number}\"\n",
    "    frameborder=\"0\"\n",
    "    width=\"1150\"\n",
    "    height=\"683\"\n",
    "  ></iframe></div>\n",
    "  ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83d42aa4-84b0-41ce-af54-e80405fa76be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_active_streams(start_with = \"\"):\n",
    "    return [s for s in spark.streams.active if len(start_with) == 0 or (s.name is not None and s.name.startswith(start_with))]\n",
    "  \n",
    "# Function to stop all streaming queries \n",
    "def stop_all_streams(start_with = \"\", sleep_time=0):\n",
    "  import time\n",
    "  time.sleep(sleep_time)\n",
    "  streams = get_active_streams(start_with)\n",
    "  if len(streams) > 0:\n",
    "    print(f\"Stopping {len(streams)} streams\")\n",
    "    for s in streams:\n",
    "        try:\n",
    "            s.stop()\n",
    "        except:\n",
    "            pass\n",
    "    print(f\"All stream stopped {'' if len(start_with) == 0 else f'(starting with: {start_with}.)'}\")\n",
    "    \n",
    "def wait_for_all_stream(start = \"\"):\n",
    "  import time\n",
    "  actives = get_active_streams(start)\n",
    "  if len(actives) > 0:\n",
    "    print(f\"{len(actives)} streams still active, waiting... ({[s.name for s in actives]})\")\n",
    "  while len(actives) > 0:\n",
    "    spark.streams.awaitAnyTermination()\n",
    "    time.sleep(1)\n",
    "    actives = get_active_streams(start)\n",
    "  print(\"All streams completed.\")\n",
    "  \n",
    "def wait_for_table(table_name, timeout_duration=120):\n",
    "  import time\n",
    "  i = 0\n",
    "  while not spark._jsparkSession.catalog().tableExists(table_name) or spark.table(table_name).count() == 0:\n",
    "    time.sleep(1)\n",
    "    if i > timeout_duration:\n",
    "      raise Exception(f\"couldn't find table {table_name} or table is empty. Do you have data being generated to be consumed?\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15755022-bc79-4b70-9f40-c85f08074229",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import mlflow\n",
    "\n",
    "import databricks\n",
    "from datetime import datetime\n",
    "\n",
    "def get_automl_run(name):\n",
    "  #get the most recent automl run\n",
    "  df = spark.table(\"hive_metastore.dbdemos_metadata.automl_experiment\").filter(col(\"name\") == name).orderBy(col(\"date\").desc()).limit(1)\n",
    "  return df.collect()\n",
    "\n",
    "#Get the automl run information from the hive_metastore.dbdemos_metadata.automl_experiment table. \n",
    "#If it's not available in the metadata table, start a new run with the given parameters\n",
    "def get_automl_run_or_start(name, model_name, dataset, target_col, timeout_minutes, move_to_production = False):\n",
    "  spark.sql(\"create database if not exists hive_metastore.dbdemos_metadata\")\n",
    "  spark.sql(\"create table if not exists hive_metastore.dbdemos_metadata.automl_experiment (name string, date string)\")\n",
    "  result = get_automl_run(name)\n",
    "  if len(result) == 0:\n",
    "    print(\"No run available, start a new Auto ML run, this will take a few minutes...\")\n",
    "    start_automl_run(name, model_name, dataset, target_col, timeout_minutes, move_to_production)\n",
    "    return (False, get_automl_run(name))\n",
    "  return (True, result[0])\n",
    "\n",
    "\n",
    "#Start a new auto ml classification task and save it as metadata.\n",
    "def start_automl_run(name, model_name, dataset, target_col, timeout_minutes = 5, move_to_production = False):\n",
    "  from databricks import automl\n",
    "  automl_run = databricks.automl.classify(\n",
    "    dataset = dataset,\n",
    "    target_col = target_col,\n",
    "    timeout_minutes = timeout_minutes\n",
    "  )\n",
    "  experiment_id = automl_run.experiment.experiment_id\n",
    "  path = automl_run.experiment.name\n",
    "  data_run_id = mlflow.search_runs(experiment_ids=[automl_run.experiment.experiment_id], filter_string = \"tags.mlflow.source.name='Notebook: DataExploration'\").iloc[0].run_id\n",
    "  exploration_notebook_id = automl_run.experiment.tags[\"_databricks_automl.exploration_notebook_id\"]\n",
    "  best_trial_notebook_id = automl_run.experiment.tags[\"_databricks_automl.best_trial_notebook_id\"]\n",
    "\n",
    "  cols = [\"name\", \"date\", \"experiment_id\", \"experiment_path\", \"data_run_id\", \"best_trial_run_id\", \"exploration_notebook_id\", \"best_trial_notebook_id\"]\n",
    "  spark.createDataFrame(data=[(name, datetime.today().isoformat(), experiment_id, path, data_run_id, automl_run.best_trial.mlflow_run_id, exploration_notebook_id, best_trial_notebook_id)], schema = cols).write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\"hive_metastore.dbdemos_metadata.automl_experiment\")\n",
    "  #Create & save the first model version in the MLFlow repo (required to setup hooks etc)\n",
    "  model_registered = mlflow.register_model(f\"runs:/{automl_run.best_trial.mlflow_run_id}/model\", model_name)\n",
    "  set_experiment_permission(path)\n",
    "  if move_to_production:\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    print(\"registering model version \"+model_registered.version+\" as production model\")\n",
    "    client.transition_model_version_stage(name = model_name, version = model_registered.version, stage = \"Production\", archive_existing_versions=True)\n",
    "  return get_automl_run(name)\n",
    "\n",
    "#Generate nice link for the given auto ml run\n",
    "def display_automl_link(name, model_name, dataset, target_col, timeout_minutes = 5, move_to_production = False):\n",
    "  from_cache, r = get_automl_run_or_start(name, model_name, dataset, target_col, timeout_minutes, move_to_production)\n",
    "  if from_cache:\n",
    "    html = f\"\"\"For exploratory data analysis, open the <a href=\"/#notebook/{r[\"exploration_notebook_id\"]}\">data exploration notebook</a><br/><br/>\"\"\"\n",
    "    html += f\"\"\"To view the best performing model, open the <a href=\"/#notebook/{r[\"best_trial_notebook_id\"]}\">best trial notebook</a><br/><br/>\"\"\"\n",
    "    html += f\"\"\"To view details about all trials, navigate to the <a href=\"/#mlflow/experiments/{r[\"experiment_id\"]}/s?orderByKey=metrics.%60val_f1_score%60&orderByAsc=false\">MLflow experiment</>\"\"\"\n",
    "    displayHTML(html)\n",
    "\n",
    "def reset_automl_run(model_name):\n",
    "  if spark._jsparkSession.catalog().tableExists('hive_metastore.dbdemos_metadata.automl_experiment'):\n",
    "      spark.sql(f\"delete from hive_metastore.dbdemos_metadata.automl_experiment where name='{model_name}'\")\n",
    "\n",
    "#Once the automl experiment is created, we assign CAN MANAGE to all users as it's shared in the workspace\n",
    "def set_experiment_permission(experiment_path):\n",
    "  url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().extraContext().apply(\"api_url\")\n",
    "  import requests\n",
    "  pat_token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "  headers =  {\"Authorization\": \"Bearer \" + pat_token, 'Content-type': 'application/json'}\n",
    "  status = requests.get(url+\"/api/2.0/workspace/get-status\", params = {\"path\": experiment_path}, headers=headers).json()\n",
    "  #Set can manage to all users to the experiment we created as it's shared among all\n",
    "  params = {\"access_control_list\": [{\"group_name\": \"users\",\"permission_level\": \"CAN_MANAGE\"}]}\n",
    "  permissions = requests.patch(f\"{url}/api/2.0/permissions/experiments/{status['object_id']}\", json = params, headers=headers)\n",
    "  if permissions.status_code != 200:\n",
    "    print(\"ERROR: couldn't set permission to all users to the autoML experiment\")\n",
    "\n",
    "  #try to find the experiment id\n",
    "  result = re.search(r\"_([a-f0-9]{8}_[a-f0-9]{4}_[a-f0-9]{4}_[a-f0-9]{4}_[a-f0-9]{12})_\", experiment_path)\n",
    "  if result is not None and len(result.groups()) > 0:\n",
    "    ex_id = result.group(0)\n",
    "  else:\n",
    "    print(experiment_path)\n",
    "    ex_id = experiment_path[experiment_path.rfind('/')+1:]\n",
    "\n",
    "  path = experiment_path\n",
    "  path = path[:path.rfind('/')]+\"/\"\n",
    "  #List to get the folder with the notebooks from the experiment\n",
    "  folders = requests.get(url+\"/api/2.0/workspace/list\", params = {\"path\": path}, headers=headers).json()\n",
    "  for f in folders['objects']:\n",
    "    if f['object_type'] == 'DIRECTORY' and ex_id in f['path']:\n",
    "        #Set the permission of the experiment notebooks to all\n",
    "        permissions = requests.patch(f\"{url}/api/2.0/permissions/directories/{f['object_id']}\", json = params, headers=headers)\n",
    "        if permissions.status_code != 200:\n",
    "          print(\"ERROR: couldn't set permission to all users to the autoML experiment notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce7509a0-1706-4679-b7d4-b24f309976ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Deprecated. TODO: remove all occurence with is_folder_empty\n",
    "def test_not_empty_folder(folder):\n",
    "  try:\n",
    "    return len(dbutils.fs.ls(folder)) > 0\n",
    "  except:\n",
    "    return False\n",
    "\n",
    "#Return true if the folder is empty or does not exists\n",
    "def is_folder_empty(folder):\n",
    "  try:\n",
    "    return len(dbutils.fs.ls(folder)) == 0\n",
    "  except:\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "615c104f-3ad4-4599-b5bf-adf903ae5696",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore', SyntaxWarning)\n",
    "    warnings.simplefilter('ignore', DeprecationWarning)\n",
    "    warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb0ee0df-d495-42aa-8ece-2d89e298768f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import collections\n",
    "import os\n",
    "def download_file(url, destination):\n",
    "    local_filename = url.split('/')[-1]\n",
    "    # NOTE the stream=True parameter below\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        print('saving '+destination+'/'+local_filename)\n",
    "        with open(destination+'/'+local_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                # If you have chunk encoded response uncomment if\n",
    "                # and set chunk_size parameter to None.\n",
    "                #if chunk: \n",
    "                f.write(chunk)\n",
    "    return local_filename\n",
    "  \n",
    "def download_file_from_git(dest, owner, repo, path):\n",
    "    if not os.path.exists(dest):\n",
    "      os.makedirs(dest)\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    files = requests.get(f'https://api.github.com/repos/{owner}/{repo}/contents{path}').json()\n",
    "    files = [f['download_url'] for f in files if 'NOTICE' not in f['name']]\n",
    "    def download_to_dest(url):\n",
    "         download_file(url, dest)\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        collections.deque(executor.map(download_to_dest, files))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00-global-setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
