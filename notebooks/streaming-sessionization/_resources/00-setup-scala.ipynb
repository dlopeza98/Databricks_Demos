{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6cdc404-ea4a-4a6e-801f-622aa2955a0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"reset_all_data\", \"false\", Seq(\"true\", \"false\"), \"Reset all data\")\n",
    "dbutils.widgets.text(\"db_prefix\", \"retail\", \"Database prefix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0d28177-4861-458e-b1b4-51bfac739540",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import scala.util.Try\n",
    "import scala.annotation.tailrec\n",
    "import org.apache.spark.sql.streaming.{StreamingQuery, Trigger}\n",
    "\n",
    "\n",
    "def getActiveStreams(startWith: String = \"\"): Seq[StreamingQuery] = {\n",
    "  spark.streams.active.filter(startWith.isEmpty || _.name.startsWith(startWith))\n",
    "}\n",
    "\n",
    "def stopAllStreams(startWith:String = \"\", sleepTime:Int = 0): Unit = {\n",
    "  Thread.sleep(sleepTime)\n",
    "  val streams = getActiveStreams(startWith)\n",
    "  if (streams.nonEmpty) {\n",
    "      println(s\"Stopping ${streams.length} streams\")\n",
    "      streams.foreach { s => Try(s.stop()).toOption }\n",
    "      val streamDescr = if (startWith.isEmpty) \"streams\" else s\"streams starting with: $startWith\"\n",
    "      println(s\"All $streamDescr stopped.\")\n",
    "  }\n",
    "}\n",
    "\n",
    "def waitForAllStreams(startWith: String = \"\"): Unit = {\n",
    "  @tailrec\n",
    "  def stopStreams(streams: Seq[StreamingQuery]): Unit = {\n",
    "    if (streams.nonEmpty) {\n",
    "      println(s\"${streams.length} streams still active, waiting... (${streams.map(_.name).mkString(\", \")})\")\n",
    "      spark.streams.awaitAnyTermination(timeoutMs=1000)\n",
    "      stopStreams(streams)\n",
    "    } else println(\"All streams completed.\")\n",
    "  }\n",
    "  stopStreams(getActiveStreams(startWith))\n",
    "}\n",
    "\n",
    "def waitForTable(tableName: String, timeoutDuration: Int = 120): Unit = {\n",
    "  (1 to timeoutDuration).foreach { _ =>\n",
    "    val tablePending = !spark.catalog.tableExists(tableName) || spark.table(tableName).count() == 0\n",
    "    if (tablePending) Thread.sleep(1000) else return\n",
    "  }\n",
    "  throw new Exception(s\"couldn't find table $tableName or table is empty. Do you have data being generated to be consumed?\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8260d9aa-8129-4057-a7d4-74275f56b6ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "val currentUser = dbutils.notebook.getContext.tags(\"user\")\n",
    "val currentUserNoAt = currentUser.split(\"@\").head.replaceAll(\"\\\\W+\", \"_\")\n",
    "val dbPrefix = dbutils.widgets.get(\"db_prefix\")\n",
    "\n",
    "val dbName = s\"${dbPrefix}_$currentUserNoAt\"\n",
    "var cloudStoragePath = s\"/Users/$currentUser/demos/$dbPrefix/scala\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5e9906e-bdbc-4976-bc13-773e058a9e8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "cloudStoragePath = s\"$cloudStoragePath/sessions\"\n",
    "\n",
    "// Reduce parallelism as we have just a few messages being produced\n",
    "spark.conf.set(\"spark.default.parallelism\", \"12\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"12\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "scala",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00-setup-scala",
   "widgets": {}
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
