{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "452d1b44-b5c4-4352-8bdf-2fe2d6838dad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# init notebook setting up the backend. \n",
    "\n",
    "Do not edit the notebook, it contains import and helpers for the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e336e66e-5987-43d2-b380-3d54fd0b48e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"reset_all_data\", \"false\", [\"true\", \"false\"], \"Reset all data\")\n",
    "reset_all_data = dbutils.widgets.get(\"reset_all_data\") == \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b35e341e-1a41-445b-b5ba-38d069f80b70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"main\"\n",
    "main_naming = \"dbdemos_fs_travel\"\n",
    "schema = dbName = db = \"dbdemos_fs_travel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f4036c8-f83e-4d6f-a37a-f2ec5cf559ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./00-global-setup-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ec43e21-5245-4b14-8080-53861d34f2fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DBDemos.setup_schema(catalog, db, reset_all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da6e4a1e-812e-45b9-81a4-c748c16d7100",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk.service.serving import (\n",
    "    EndpointCoreConfigInput,\n",
    "    ServedModelInput,\n",
    "    ServedModelInputWorkloadSize,\n",
    "    ServingEndpointDetailed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2738c7f6-2bd4-406e-8f8b-aae82481d867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# *****\n",
    "# Loading Modules\n",
    "# *****\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.window as w\n",
    "import timeit\n",
    "from databricks import feature_store\n",
    "from databricks.feature_store import feature_table, FeatureLookup\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "import uuid\n",
    "import os\n",
    "import requests\n",
    "\n",
    "from pyspark.sql.functions import lit, expr, rand, col, count, mean, unix_timestamp, window, when\n",
    "from pyspark.sql.types import StringType, DoubleType, IntegerType, LongType\n",
    "import numpy as np\n",
    "\n",
    "#Add features from the time variable \n",
    "def add_time_features_spark(df):\n",
    "    return add_time_features(df.pandas_api()).to_spark()\n",
    "\n",
    "def add_time_features(df):\n",
    "    # Extract day of the week, day of the month, and hour from the ts column\n",
    "    df['day_of_week'] = df['ts'].dt.dayofweek\n",
    "    df['day_of_month'] = df['ts'].dt.day\n",
    "    df['hour'] = df['ts'].dt.hour\n",
    "    \n",
    "    # Calculate sin and cos values for the day of the week, day of the month, and hour\n",
    "    df['day_of_week_sin'] = np.sin(df['day_of_week'] * (2 * np.pi / 7))\n",
    "    df['day_of_week_cos'] = np.cos(df['day_of_week'] * (2 * np.pi / 7))\n",
    "    df['day_of_month_sin'] = np.sin(df['day_of_month'] * (2 * np.pi / 30))\n",
    "    df['day_of_month_cos'] = np.cos(df['day_of_month'] * (2 * np.pi / 30))\n",
    "    df['hour_sin'] = np.sin(df['hour'] * (2 * np.pi / 24))\n",
    "    df['hour_cos'] = np.cos(df['hour'] * (2 * np.pi / 24))\n",
    "    df = df.drop(['day_of_week', 'day_of_month', 'hour'], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f0500e6-9ee9-4e15-8eba-3b1b8ded879f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def online_table_exists(table_name):\n",
    "    w = WorkspaceClient()\n",
    "    try:\n",
    "        w.online_tables.get(name=table_name)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return 'already exists' in str(e)\n",
    "    return False\n",
    "  \n",
    "def wait_for_online_tables(catalog, schema, tables, waiting_time = 300):\n",
    "    sleep_time = 10\n",
    "    import time\n",
    "    from databricks.sdk import WorkspaceClient\n",
    "    w = WorkspaceClient()\n",
    "    for table in tables:\n",
    "        for i in range(int(waiting_time/sleep_time)):\n",
    "            state = w.online_tables.get(name=f\"{catalog}.{db}.{table}\").status.detailed_state.value\n",
    "            if state.startswith('ONLINE'):\n",
    "                print(f'Table {table} online: {state}')\n",
    "                break\n",
    "            time.sleep(sleep_time)\n",
    "            \n",
    "def delete_fs(fs_table_name):\n",
    "  print(\"Deleting Feature Table\", fs_table_name)\n",
    "  try:\n",
    "    fs = feature_store.FeatureStoreClient()\n",
    "    fs.drop_table(name=fs_table_name)\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {fs_table_name}\")\n",
    "  except Exception as e:\n",
    "    print(\"Can't delete table, likely not existing: \"+str(e))  \n",
    "\n",
    "def delete_fss(catalog, db, tables):\n",
    "  for table in tables:\n",
    "    delete_fs(f\"{catalog}.{db}.{table}\")\n",
    "\n",
    "def get_last_model_version(model_full_name):\n",
    "    mlflow_client = MlflowClient(registry_uri=\"databricks-uc\")\n",
    "    # Use the MlflowClient to get a list of all versions for the registered model in Unity Catalog\n",
    "    all_versions = mlflow_client.search_model_versions(f\"name='{model_full_name}'\")\n",
    "    # Sort the list of versions by version number and get the latest version\n",
    "    latest_version = max([int(v.version) for v in all_versions])\n",
    "    # Use the MlflowClient to get the latest version of the registered model in Unity Catalog\n",
    "    return mlflow_client.get_model_version(model_full_name, str(latest_version))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44f7e562-93fd-4cd9-92c1-69ed547cac99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "from databricks.automl_runtime.sklearn.column_selector import ColumnSelector\n",
    "import databricks.automl_runtime\n",
    "\n",
    "\n",
    "import lightgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "import pandas as pd\n",
    "\n",
    "params = {\n",
    "  \"colsample_bytree\": 0.40,\n",
    "  \"lambda_l1\": 0.15,\n",
    "  \"lambda_l2\": 2.1,\n",
    "  \"learning_rate\": 4.1,\n",
    "  \"max_bin\": 14,\n",
    "  \"max_depth\": 12,\n",
    "  \"min_child_samples\": 182,\n",
    "  \"n_estimators\": 100,\n",
    "  \"num_leaves\": 790,\n",
    "  \"path_smooth\": 68.0,\n",
    "  \"subsample\": 0.52,\n",
    "  \"random_state\": 607,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfdf3079-da84-48c9-93f7-cb19559e703e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "travel_purchase_df = spark.read.option(\"inferSchema\", \"true\").load(\"/databricks-datasets/travel_recommendations_realtime/raw_travel_data/fs-demo_vacation-purchase_logs/\", format=\"csv\", header=\"true\")\n",
    "travel_purchase_df = travel_purchase_df.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "travel_purchase_df.withColumn(\"booking_date\", F.col(\"booking_date\").cast('date')).write.mode('overwrite').saveAsTable('travel_purchase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3795b61b-1493-4c55-b1aa-697dd66c3f33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore', SyntaxWarning)\n",
    "    warnings.simplefilter('ignore', DeprecationWarning)\n",
    "    warnings.simplefilter('ignore', UserWarning)\n",
    "    warnings.simplefilter('ignore', FutureWarning)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00-init-basic",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
