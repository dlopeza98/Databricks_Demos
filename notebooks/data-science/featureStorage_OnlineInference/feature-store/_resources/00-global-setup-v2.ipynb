{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1aa715d-0306-4641-8207-b1034295b822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Technical Setup notebook. Hide this cell results\n",
    "Initialize dataset to the current user and cleanup data when reset_all_data is set to true\n",
    "\n",
    "Do not edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "975f0768-033d-4988-b86c-427c45081479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"reset_all_data\", \"false\", [\"true\", \"false\"], \"Reset all data\")\n",
    "dbutils.widgets.text(\"min_dbr_version\", \"12.2\", \"Min required DBR version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d800ba7-9fec-4240-939a-103b1ebc464e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import collections\n",
    "import os\n",
    "\n",
    "\n",
    "class DBDemos():\n",
    "  @staticmethod\n",
    "  def setup_schema(catalog, db, reset_all_data, volume_name = None):\n",
    "    if reset_all_data:\n",
    "      print(f'clearing up volume named `{catalog}`.`{db}`.`{volume_name}`')\n",
    "      try:\n",
    "        spark.sql(f\"DROP VOLUME IF EXISTS `{catalog}`.`{db}`.`{volume_name}`\")\n",
    "        spark.sql(f\"DROP SCHEMA IF EXISTS `{catalog}`.`{db}` CASCADE\")\n",
    "      except Exception as e:\n",
    "        print(f'catalog `{catalog}` or schema `{db}` do not exist.  Skipping data reset')\n",
    "\n",
    "    def use_and_create_db(catalog, dbName, cloud_storage_path = None):\n",
    "      print(f\"USE CATALOG `{catalog}`\")\n",
    "      spark.sql(f\"USE CATALOG `{catalog}`\")\n",
    "      spark.sql(f\"\"\"create database if not exists `{dbName}` \"\"\")\n",
    "\n",
    "    assert catalog not in ['hive_metastore', 'spark_catalog'], \"This demo only support Unity. Please change your catalog name.\"\n",
    "    #If the catalog is defined, we force it to the given value and throw exception if not.\n",
    "    current_catalog = spark.sql(\"select current_catalog()\").collect()[0]['current_catalog()']\n",
    "    if current_catalog != catalog:\n",
    "      catalogs = [r['catalog'] for r in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "      if catalog not in catalogs:\n",
    "        spark.sql(f\"CREATE CATALOG IF NOT EXISTS `{catalog}`\")\n",
    "        if catalog == 'dbdemos':\n",
    "          spark.sql(f\"ALTER CATALOG `{catalog}` OWNER TO `account users`\")\n",
    "    use_and_create_db(catalog, db)\n",
    "\n",
    "    if catalog == 'dbdemos':\n",
    "      try:\n",
    "        spark.sql(f\"GRANT CREATE, USAGE on DATABASE `{catalog}`.`{db}` TO `account users`\")\n",
    "        spark.sql(f\"ALTER SCHEMA `{catalog}`.`{db}` OWNER TO `account users`\")\n",
    "        for t in spark.sql(f'SHOW TABLES in {catalog}.{db}').collect():\n",
    "          try:\n",
    "            spark.sql(f'GRANT ALL PRIVILEGES ON TABLE {catalog}.{db}.{t[\"tableName\"]} TO `account users`')\n",
    "            spark.sql(f'ALTER TABLE {catalog}.{db}.{t[\"tableName\"]} OWNER TO `account users`')\n",
    "          except Exception as e:\n",
    "            if \"NOT_IMPLEMENTED.TRANSFER_MATERIALIZED_VIEW_OWNERSHIP\" not in str(e) and \"STREAMING_TABLE_OPERATION_NOT_ALLOWED.UNSUPPORTED_OPERATION\" not in str(e) :\n",
    "              print(f'WARN: Couldn t set table {catalog}.{db}.{t[\"tableName\"]} owner to account users, error: {e}')\n",
    "      except Exception as e:\n",
    "        print(\"Couldn't grant access to the schema to all users:\"+str(e))    \n",
    "\n",
    "    print(f\"using catalog.database `{catalog}`.`{db}`\")\n",
    "    spark.sql(f\"\"\"USE `{catalog}`.`{db}`\"\"\")    \n",
    "\n",
    "    if volume_name:\n",
    "      spark.sql(f'CREATE VOLUME IF NOT EXISTS {volume_name};')\n",
    "\n",
    "                     \n",
    "  #Return true if the folder is empty or does not exists\n",
    "  @staticmethod\n",
    "  def is_folder_empty(folder):\n",
    "    try:\n",
    "      return len(dbutils.fs.ls(folder)) == 0\n",
    "    except:\n",
    "      return True\n",
    "    \n",
    "  @staticmethod\n",
    "  def is_any_folder_empty(folders):\n",
    "    return any([DBDemos.is_folder_empty(f) for f in folders])\n",
    "\n",
    "  @staticmethod\n",
    "  def set_model_permission(model_name, permission, principal):\n",
    "    import databricks.sdk.service.catalog as c\n",
    "    sdk_client = databricks.sdk.WorkspaceClient()\n",
    "    return sdk_client.grants.update(c.SecurableType.FUNCTION, model_name, changes=[\n",
    "                              c.PermissionsChange(add=[c.Privilege[permission]], principal=principal)])\n",
    "\n",
    "  @staticmethod\n",
    "  def set_model_endpoint_permission(endpoint_name, permission, group_name):\n",
    "    import databricks.sdk.service.serving as s\n",
    "    sdk_client = databricks.sdk.WorkspaceClient()\n",
    "    ep = sdk_client.serving_endpoints.get(endpoint_name)\n",
    "    return sdk_client.serving_endpoints.set_permissions(serving_endpoint_id=ep.id, access_control_list=[s.ServingEndpointAccessControlRequest(permission_level=s.ServingEndpointPermissionLevel[permission], group_name=group_name)])\n",
    "\n",
    "  @staticmethod\n",
    "  def set_index_permission(index_name, permission, principal):\n",
    "      import databricks.sdk.service.catalog as c\n",
    "      sdk_client = databricks.sdk.WorkspaceClient()\n",
    "      return sdk_client.grants.update(c.SecurableType.TABLE, index_name, changes=[\n",
    "                              c.PermissionsChange(add=[c.Privilege[permission]], principal=principal)])\n",
    "    \n",
    "\n",
    "  @staticmethod\n",
    "  def download_file_from_git(dest, owner, repo, path):\n",
    "    def download_file(url, destination):\n",
    "      local_filename = url.split('/')[-1]\n",
    "      # NOTE the stream=True parameter below\n",
    "      with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        print('saving '+destination+'/'+local_filename)\n",
    "        with open(destination+'/'+local_filename, 'wb') as f:\n",
    "          for chunk in r.iter_content(chunk_size=8192): \n",
    "            # If you have chunk encoded response uncomment if\n",
    "            # and set chunk_size parameter to None.\n",
    "            #if chunk: \n",
    "            f.write(chunk)\n",
    "      return local_filename\n",
    "\n",
    "    if not os.path.exists(dest):\n",
    "      os.makedirs(dest)\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    files = requests.get(f'https://api.github.com/repos/{owner}/{repo}/contents{path}').json()\n",
    "    files = [f['download_url'] for f in files if 'NOTICE' not in f['name']]\n",
    "    def download_to_dest(url):\n",
    "      try:\n",
    "        #Temporary fix to avoid hitting github limits - Swap github to our S3 bucket to download files\n",
    "        s3url = url.replace(\"https://raw.githubusercontent.com/databricks-demos/dbdemos-dataset/main/\", \"https://notebooks.databricks.com/demos/dbdemos-dataset/\")\n",
    "        download_file(s3url, dest)\n",
    "      except:\n",
    "        download_file(url, dest)\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "      collections.deque(executor.map(download_to_dest, files))\n",
    "         \n",
    "\n",
    "  #force the experiment to the field demos one. Required to launch as a batch\n",
    "  @staticmethod\n",
    "  def init_experiment_for_batch(demo_name, experiment_name):\n",
    "    import mlflow\n",
    "    #You can programatically get a PAT token with the following\n",
    "    from databricks.sdk import WorkspaceClient\n",
    "    w = WorkspaceClient()\n",
    "    xp_root_path = f\"/Shared/dbdemos/experiments/{demo_name}\"\n",
    "    try:\n",
    "      r = w.workspace.mkdirs(path=xp_root_path)\n",
    "    except Exception as e:\n",
    "      print(f\"ERROR: couldn't create a folder for the experiment under {xp_root_path} - please create the folder manually or  skip this init (used for job only: {e})\")\n",
    "      raise e\n",
    "    xp = f\"{xp_root_path}/{experiment_name}\"\n",
    "    print(f\"Using common experiment under {xp}\")\n",
    "    mlflow.set_experiment(xp)\n",
    "    DBDemos.set_experiment_permission(xp)\n",
    "    return mlflow.get_experiment_by_name(xp)\n",
    "\n",
    "  @staticmethod\n",
    "  def set_experiment_permission(experiment_path):\n",
    "    from databricks.sdk import WorkspaceClient\n",
    "    from databricks.sdk.service import iam\n",
    "    w = WorkspaceClient()\n",
    "    try:\n",
    "      status = w.workspace.get_status(experiment_path)\n",
    "      w.permissions.set(\"experiments\", request_object_id=status.object_id,  access_control_list=[\n",
    "                            iam.AccessControlRequest(group_name=\"users\", permission_level=iam.PermissionLevel.CAN_MANAGE)])    \n",
    "    except Exception as e:\n",
    "      print(f\"error setting up shared experiment {experiment_path} permission: {e}\")\n",
    "\n",
    "    print(f\"Experiment on {experiment_path} was set public\")\n",
    "\n",
    "\n",
    "  @staticmethod\n",
    "  def get_active_streams(start_with = \"\"):\n",
    "    return [s for s in spark.streams.active if len(start_with) == 0 or (s.name is not None and s.name.startswith(start_with))]\n",
    "\n",
    "  @staticmethod\n",
    "  def stop_all_streams_asynch(start_with = \"\", sleep_time=0):\n",
    "    import threading\n",
    "    def stop_streams():\n",
    "        DBDemos.stop_all_streams(start_with=start_with, sleep_time=sleep_time)\n",
    "\n",
    "    thread = threading.Thread(target=stop_streams)\n",
    "    thread.start()\n",
    "\n",
    "  @staticmethod\n",
    "  def stop_all_streams(start_with = \"\", sleep_time=0):\n",
    "    import time\n",
    "    time.sleep(sleep_time)\n",
    "    streams = DBDemos.get_active_streams(start_with)\n",
    "    if len(streams) > 0:\n",
    "      print(f\"Stopping {len(streams)} streams\")\n",
    "      for s in streams:\n",
    "          try:\n",
    "              s.stop()\n",
    "          except:\n",
    "              pass\n",
    "      print(f\"All stream stopped {'' if len(start_with) == 0 else f'(starting with: {start_with}.)'}\")\n",
    "\n",
    "  @staticmethod\n",
    "  def wait_for_all_stream(start = \"\"):\n",
    "    import time\n",
    "    actives = DBDemos.get_active_streams(start)\n",
    "    if len(actives) > 0:\n",
    "      print(f\"{len(actives)} streams still active, waiting... ({[s.name for s in actives]})\")\n",
    "    while len(actives) > 0:\n",
    "      spark.streams.awaitAnyTermination()\n",
    "      time.sleep(1)\n",
    "      actives = DBDemos.get_active_streams(start)\n",
    "    print(\"All streams completed.\")\n",
    "\n",
    "  @staticmethod\n",
    "  def get_last_experiment(demo_name, experiment_path = \"/Shared/dbdemos/experiments/\"):\n",
    "    import requests\n",
    "    import re\n",
    "    from datetime import datetime\n",
    "    #TODO: waiting for https://github.com/databricks/databricks-sdk-py/issues/509 to use the python sdk instead\n",
    "    base_url =dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "    token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n",
    "    r = requests.get(base_url+\"/api/2.0/workspace/list\", params={'path': f\"{experiment_path}/{demo_name}\"}, headers=headers).json()\n",
    "    if 'objects' not in r:\n",
    "      raise Exception(f\"No experiment available for this demo. Please re-run the previous notebook with the AutoML run. - {r}\")\n",
    "    xps = [f for f in r['objects'] if f['object_type'] == 'MLFLOW_EXPERIMENT' and 'automl' in f['path']]\n",
    "    xps = [x for x in xps if re.search(r'(\\d{4}-\\d{2}-\\d{2}_\\d{2}:\\d{2}:\\d{2})', x['path'])]\n",
    "    sorted_xp = sorted(xps, key=lambda f: f['path'], reverse = True)\n",
    "    if len(sorted_xp) == 0:\n",
    "      raise Exception(f\"No experiment available for this demo. Please re-run the previous notebook with the AutoML run. - {r}\")\n",
    "\n",
    "    last_xp = sorted_xp[0]\n",
    "\n",
    "    # Search for the date pattern in the input string\n",
    "    match = re.search(r'(\\d{4}-\\d{2}-\\d{2}_\\d{2}:\\d{2}:\\d{2})', last_xp['path'])\n",
    "\n",
    "    if match:\n",
    "        date_str = match.group(1)  # Extract the matched date string\n",
    "        date = datetime.strptime(date_str, '%Y-%m-%d_%H:%M:%S')  # Convert to a datetime object\n",
    "        # Calculate the difference in days from the current date\n",
    "        days_difference = (datetime.now() - date).days\n",
    "        if days_difference > 30:\n",
    "            raise Exception(f\"It looks like the last experiment {last_xp} is too old ({days_difference} days). Please re-run the previous notebook to make sure you have the latest version. Delete the experiment folder if needed to clear history.\")\n",
    "    else:\n",
    "        raise Exception(f\"Invalid experiment format or no experiment available. Please re-run the previous notebook. {last_xp['path']}\")\n",
    "    return last_xp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1def1ae9-9a8f-4026-9ff1-6f31e3d31757",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Let's skip some warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00-global-setup-v2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
