{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eeee6b08-ef27-48ea-b22f-2d9b9a6b5dfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Do not edit the notebook, it contains import and helpers for the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32d81719-c030-4e07-9324-2c02a97a7356",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./00-init-basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "149d4c0b-b6c8-4ab2-b7b3-294211718241",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "def create_user_features(travel_purchase_df):\n",
    "    \"\"\"\n",
    "    Computes the user_features feature group.\n",
    "    \"\"\"\n",
    "    travel_purchase_df = travel_purchase_df.withColumn('ts_l', F.col(\"ts\").cast(\"long\"))\n",
    "    travel_purchase_df = (\n",
    "        # Sum total purchased for 7 days\n",
    "        travel_purchase_df.withColumn(\"lookedup_price_7d_rolling_sum\",\n",
    "            F.sum(\"price\").over(w.Window.partitionBy(\"user_id\").orderBy(F.col(\"ts_l\")).rangeBetween(start=-(7 * 86400), end=0))\n",
    "        )\n",
    "        # counting number of purchases per week\n",
    "        .withColumn(\"lookups_7d_rolling_sum\", \n",
    "            F.count(\"*\").over(w.Window.partitionBy(\"user_id\").orderBy(F.col(\"ts_l\")).rangeBetween(start=-(7 * 86400), end=0))\n",
    "        )\n",
    "        # total price 7d / total purchases for 7 d \n",
    "        .withColumn(\"mean_price_7d\",  F.col(\"lookedup_price_7d_rolling_sum\") / F.col(\"lookups_7d_rolling_sum\"))\n",
    "         # converting True / False into 1/0\n",
    "        .withColumn(\"tickets_purchased\", F.col(\"purchased\").cast('int'))\n",
    "        # how many purchases for the past 6m\n",
    "        .withColumn(\"last_6m_purchases\", \n",
    "            F.sum(\"tickets_purchased\").over(w.Window.partitionBy(\"user_id\").orderBy(F.col(\"ts_l\")).rangeBetween(start=-(6 * 30 * 86400), end=0))\n",
    "        )\n",
    "        .select(\"user_id\", \"ts\", \"mean_price_7d\", \"last_6m_purchases\", \"user_longitude\", \"user_latitude\")\n",
    "    )\n",
    "    return travel_purchase_df\n",
    "\n",
    "\n",
    "\n",
    "def destination_features_fn(travel_purchase_df):\n",
    "    \"\"\"\n",
    "    Computes the destination_features feature group.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        travel_purchase_df\n",
    "          .withColumn(\"clicked\", F.col(\"clicked\").cast(\"int\"))\n",
    "          .withColumn(\"sum_clicks_7d\", \n",
    "            F.sum(\"clicked\").over(w.Window.partitionBy(\"destination_id\").orderBy(F.col(\"ts\").cast(\"long\")).rangeBetween(start=-(7 * 86400), end=0))\n",
    "          )\n",
    "          .withColumn(\"sum_impressions_7d\", \n",
    "            F.count(\"*\").over(w.Window.partitionBy(\"destination_id\").orderBy(F.col(\"ts\").cast(\"long\")).rangeBetween(start=-(7 * 86400), end=0))\n",
    "          )\n",
    "          .select(\"destination_id\", \"ts\", \"sum_clicks_7d\", \"sum_impressions_7d\")\n",
    "    )  \n",
    "\n",
    "#Required for pandas_on_spark assign to work properly\n",
    "import pyspark.pandas as ps\n",
    "import timeit\n",
    "ps.set_option('compute.ops_on_diff_frames', True)\n",
    "\n",
    "#Compute distance between 2 points. Could use geopy instead\n",
    "def compute_hearth_distance(lat1, lon1, lat2, lon2):\n",
    "  dlat, dlon = np.radians(lat2 - lat1), np.radians(lon2 - lon1)\n",
    "  a = np.sin(dlat/2)**2 + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.sin(dlon/2)**2\n",
    "  return 2 * 6371 * np.arcsin(np.sqrt(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4796083c-66e6-4929-816f-189aec875837",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Below are cleanup related functions   \n",
    "import boto3\n",
    "   \n",
    "def delete_online_store_table(table_name):\n",
    "  assert table_name.startswith('feature_store_dbdemos')\n",
    "  if get_cloud_name() == \"aws\":\n",
    "    delete_dynamodb_table(online_table_name)\n",
    "  elif get_cloud_name() == \"azure\":\n",
    "    delete_cosmosdb_container(online_table_name)\n",
    "  else:\n",
    "    raise Exception(f\"cloud not supported : {get_cloud_name()}\")\n",
    "    \n",
    "def delete_dynamodb_table(table_name):\n",
    "  #TODO: add your key \n",
    "  #AWS_DYNAMO_DB_KEY_ID = aws_access_key_id=dbutils.secrets.get(scope=\"field-all-users-feature-store-example-write\", key=\"field-eng-access-key-id\")\n",
    "  #AWS_DYNAMO_DB_KEY = aws_secret_access_key=dbutils.secrets.get(scope=\"field-all-users-feature-store-example-write\", key=\"field-eng-secret-access-key\")\n",
    "  client = boto3.client('dynamodb', aws_access_key_id=AWS_DYNAMO_DB_KEY_ID, aws_secret_access_key=AWS_DYNAMO_DB_KEY, region_name=\"us-west-2\")\n",
    "  client.delete_table(TableName=table_name)\n",
    "  waiter = client.get_waiter('table_not_exists')\n",
    "  waiter.wait(TableName=table_name)\n",
    "  print(f\"table: '{table_name}' was deleted\")\n",
    "\n",
    "from azure.cosmos import cosmos_client\n",
    "import azure.cosmos.exceptions as exceptions\n",
    "\n",
    "def delete_cosmosdb_container(container_name, account_uri_field_demo, database = \"field_demos\"):\n",
    "  #TODO: add your key\n",
    "  #COSMO_DB_KEY = dbutils.secrets.get(scope=\"feature-store-example-write\", key=\"field-eng-authorization-key\")\n",
    "  client = cosmos_client.CosmosClient(account_uri_field_demo, credential=COSMO_DB_KEY)\n",
    "  database = client.get_database_client(database)\n",
    "  container = database.get_container_client(container_name)\n",
    "  try:\n",
    "      database.delete_container(container)\n",
    "      print('Container with id \\'{0}\\' was deleted'.format(container))\n",
    "  except exceptions.CosmosResourceNotFoundError:\n",
    "      print('A container with id \\'{0}\\' does not exist'.format(container))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ff4a967-c5cf-4e85-bad4-fca47cc55d86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def func_stop_streaming_query(query):\n",
    "  import time\n",
    "  while len(query.recentProgress) == 0 or query.status[\"isDataAvailable\"]:\n",
    "    print(\"waiting for stream to process all data\")\n",
    "    print(query.status)\n",
    "    time.sleep(10)\n",
    "  query.stop() \n",
    "  print(\"Just stopped one of the streaming queries.\")\n",
    "  \n",
    "\n",
    "from mlflow.tracking.client import MlflowClient\n",
    "def get_latest_model_version(model_name: str):\n",
    "  client = MlflowClient()\n",
    "  models = client.get_latest_versions(model_name, stages=[\"None\"])\n",
    "  for m in models:\n",
    "    new_model_version = m.version\n",
    "  return new_model_version\n",
    "\n",
    "\n",
    "def cleanup(query, query2):\n",
    "  func_stop_streaming_query(query)\n",
    "  func_stop_streaming_query(query2)\n",
    "  \n",
    "  func_delete_model_serving_endpoint(model_serving_endpoint_name)\n",
    "\n",
    "  fs_table_names = [\n",
    "    fs_table_destination_popularity_features,\n",
    "    fs_table_destination_location_features,\n",
    "    fs_table_destination_availability_features,\n",
    "    fs_table_user_features\n",
    "  ]\n",
    "  fs_online_table_names = [\n",
    "    fs_online_destination_popularity_features,\n",
    "    fs_online_destination_location_features,\n",
    "    fs_online_destination_availability_features,\n",
    "    fs_online_user_features\n",
    "  ]\n",
    "  db_name = database_name\n",
    "  delta_checkpoint = fs_destination_availability_features_delta_checkpoint\n",
    "  online_checkpoint = fs_destination_availability_features_online_checkpoint\n",
    "  model = model_name\n",
    "\n",
    "  fs = feature_store.FeatureStoreClient()\n",
    "\n",
    "  for table_name in fs_table_names:\n",
    "    try:\n",
    "      fs.drop_table(name=table_name)\n",
    "      print(table_name, \"is dropped!\")\n",
    "    except Exception as ex:\n",
    "      print(ex)\n",
    "\n",
    "  try:\n",
    "    drop_database(db_name)\n",
    "  except Exception as ex:\n",
    "    print(ex)\n",
    "\n",
    "\n",
    "  for container_name in fs_online_table_names:\n",
    "    try:\n",
    "      print(\"currentlly working on this online table/container dropping: \", container_name)\n",
    "      if cloud_name == \"azure\":\n",
    "        delete_cosmosdb_container(container_name, account_uri_field_demo)\n",
    "        print(\"\\n\")\n",
    "      elif cloud_name == \"aws\":\n",
    "        delete_dynamodb_table(table_name=container_name)\n",
    "        print(\"\\n\")\n",
    "    except Exception as ex:\n",
    "      print(ex)\n",
    "\n",
    "  try:    \n",
    "    dbutils.fs.rm(\n",
    "      delta_checkpoint, True\n",
    "    )\n",
    "  except Exception as ex:\n",
    "      print(ex) \n",
    "\n",
    "  try:    \n",
    "    dbutils.fs.rm(\n",
    "      online_checkpoint, True\n",
    "    )\n",
    "  except Exception as ex:\n",
    "    print(ex)\n",
    "\n",
    "  try:\n",
    "    delete_model(model_name)\n",
    "  except Exception as ex:\n",
    "    print(ex)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96d0e794-92ff-415b-b5f9-a0a8190a6d6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "destination_location_df = spark.read.option(\"inferSchema\", \"true\").load(\"/databricks-datasets/travel_recommendations_realtime/raw_travel_data/fs-demo_destination-locations/\",  format=\"csv\", header=\"true\")\n",
    "destination_location_df.write.mode('overwrite').saveAsTable('destination_location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d679b5c-1f2f-4305-81e0-ee002c1437f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def wait_for_feature_endpoint_to_start(fe, endpoint_name: str):\n",
    "    for i in range (100):\n",
    "        ep = fe.get_feature_serving_endpoint(name=endpoint_name)\n",
    "        if ep.state == 'IN_PROGRESS':\n",
    "            if i % 10 == 0:\n",
    "                print(f\"deployment in progress, please wait for your feature serving endpoint to be deployed {ep}\")\n",
    "            time.sleep(5)\n",
    "        else:\n",
    "            if ep.state != 'READY':\n",
    "                raise Exception(f\"Endpoint is in abnormal state: {ep}\")\n",
    "            print(f\"Endpoint {endpoint_name} ready - {ep}\")\n",
    "            return ep\n",
    "        \n",
    "def get_headers():\n",
    "    token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "    return {'Authorization': f'Bearer {token}', 'Content-Type': 'application/json'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f62fc46-2e34-45fd-b279-62d8c02ace66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class EndpointApiClient:\n",
    "    def __init__(self):\n",
    "        self.base_url =dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "        self.token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "        self.headers = {\"Authorization\": f\"Bearer {self.token}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    def create_inference_endpoint(self, endpoint_name, served_models):\n",
    "        data = {\"name\": endpoint_name, \"config\": {\"served_models\": served_models}}\n",
    "        return self._post(\"api/2.0/serving-endpoints\", data)\n",
    "\n",
    "    def get_inference_endpoint(self, endpoint_name):\n",
    "        return self._get(f\"api/2.0/serving-endpoints/{endpoint_name}\", allow_error=True)\n",
    "      \n",
    "      \n",
    "    def inference_endpoint_exists(self, endpoint_name):\n",
    "      ep = self.get_inference_endpoint(endpoint_name)\n",
    "      if 'error_code' in ep and ep['error_code'] == 'RESOURCE_DOES_NOT_EXIST':\n",
    "          return False\n",
    "      if 'error_code' in ep and ep['error_code'] != 'RESOURCE_DOES_NOT_EXIST':\n",
    "          raise Exception(f\"endpoint exists ? {ep}\")\n",
    "      return True\n",
    "\n",
    "    def create_endpoint_if_not_exists(self, endpoint_name, model_name, model_version, workload_size, scale_to_zero_enabled=True, wait_start=True):\n",
    "      models = [{\n",
    "            \"model_name\": model_name,\n",
    "            \"model_version\": model_version,\n",
    "            \"workload_size\": workload_size,\n",
    "            \"scale_to_zero_enabled\": scale_to_zero_enabled,\n",
    "      }]\n",
    "      if not self.inference_endpoint_exists(endpoint_name):\n",
    "        r = self.create_inference_endpoint(endpoint_name, models)\n",
    "      #Make sure we have the proper version deployed\n",
    "      else:\n",
    "        ep = self.get_inference_endpoint(endpoint_name)\n",
    "        if 'pending_config' in ep:\n",
    "            self.wait_endpoint_start(endpoint_name)\n",
    "            ep = self.get_inference_endpoint(endpoint_name)\n",
    "        if 'pending_config' in ep:\n",
    "            model_deployed = ep['pending_config']['served_models'][0]\n",
    "            print(f\"Error with the model deployed: {model_deployed} - state {ep['state']}\")\n",
    "        else:\n",
    "            model_deployed = ep['config']['served_models'][0]\n",
    "        if model_deployed['model_version'] != model_version:\n",
    "          print(f\"Current model is version {model_deployed['model_version']}. Updating to {model_version}...\")\n",
    "          u = self.update_model_endpoint(endpoint_name, {\"served_models\": models})\n",
    "      if wait_start:\n",
    "        self.wait_endpoint_start(endpoint_name)\n",
    "      \n",
    "      \n",
    "    def list_inference_endpoints(self):\n",
    "        return self._get(\"api/2.0/serving-endpoints\")\n",
    "\n",
    "    def update_model_endpoint(self, endpoint_name, conf):\n",
    "        return self._put(f\"api/2.0/serving-endpoints/{endpoint_name}/config\", conf)\n",
    "\n",
    "    def delete_inference_endpoint(self, endpoint_name):\n",
    "        return self._delete(f\"api/2.0/serving-endpoints/{endpoint_name}\")\n",
    "\n",
    "    def wait_endpoint_start(self, endpoint_name):\n",
    "      i = 0\n",
    "      while self.get_inference_endpoint(endpoint_name)['state']['config_update'] == \"IN_PROGRESS\" and i < 500:\n",
    "        print(\"waiting for endpoint to build model image and start\")\n",
    "        time.sleep(30)\n",
    "        i += 1\n",
    "      \n",
    "    # Making predictions\n",
    "\n",
    "    def query_inference_endpoint(self, endpoint_name, data):\n",
    "        return self._post(f\"realtime-inference/{endpoint_name}/invocations\", data)\n",
    "\n",
    "    # Debugging\n",
    "\n",
    "    def get_served_model_build_logs(self, endpoint_name, served_model_name):\n",
    "        return self._get(\n",
    "            f\"api/2.0/serving-endpoints/{endpoint_name}/served-models/{served_model_name}/build-logs\"\n",
    "        )\n",
    "\n",
    "    def get_served_model_server_logs(self, endpoint_name, served_model_name):\n",
    "        return self._get(\n",
    "            f\"api/2.0/serving-endpoints/{endpoint_name}/served-models/{served_model_name}/logs\"\n",
    "        )\n",
    "\n",
    "    def get_inference_endpoint_events(self, endpoint_name):\n",
    "        return self._get(f\"api/2.0/serving-endpoints/{endpoint_name}/events\")\n",
    "\n",
    "    def _get(self, uri, data = {}, allow_error = False):\n",
    "        r = requests.get(f\"{self.base_url}/{uri}\", params=data, headers=self.headers)\n",
    "        return self._process(r, allow_error)\n",
    "\n",
    "    def _post(self, uri, data = {}, allow_error = False):\n",
    "        return self._process(requests.post(f\"{self.base_url}/{uri}\", json=data, headers=self.headers), allow_error)\n",
    "\n",
    "    def _put(self, uri, data = {}, allow_error = False):\n",
    "        return self._process(requests.put(f\"{self.base_url}/{uri}\", json=data, headers=self.headers), allow_error)\n",
    "\n",
    "    def _delete(self, uri, data = {}, allow_error = False):\n",
    "        return self._process(requests.delete(f\"{self.base_url}/{uri}\", json=data, headers=self.headers), allow_error)\n",
    "\n",
    "    def _process(self, r, allow_error = False):\n",
    "      if r.status_code == 500 or r.status_code == 403 or not allow_error:\n",
    "        print(r.text)\n",
    "        r.raise_for_status()\n",
    "      return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "185fb053-1dbb-40c4-87ac-f0c917160ad8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_active_streams(start_with = \"\"):\n",
    "    return [s for s in spark.streams.active if len(start_with) == 0 or (s.name is not None and s.name.startswith(start_with))]\n",
    "  \n",
    "  \n",
    "# Function to stop all streaming queries \n",
    "def stop_all_streams(start_with = \"\", sleep_time=0):\n",
    "  import time\n",
    "  time.sleep(sleep_time)\n",
    "  streams = get_active_streams(start_with)\n",
    "  if len(streams) > 0:\n",
    "    print(f\"Stopping {len(streams)} streams\")\n",
    "    for s in streams:\n",
    "        try:\n",
    "            s.stop()\n",
    "        except:\n",
    "            pass\n",
    "    print(f\"All stream stopped {'' if len(start_with) == 0 else f'(starting with: {start_with}.)'}\")\n",
    "\n",
    "\n",
    "def retrain_expert_model():\n",
    "  return dbutils.widgets.get('retrain_model') == 'true' or not model_exists(model_name_expert)\n",
    "\n",
    "def model_exists(model_name):\n",
    "  try:\n",
    "    client = mlflow.tracking.MlflowClient()             \n",
    "    latest_model = client.get_latest_versions(model_name)\n",
    "    return True\n",
    "  except:\n",
    "    return False"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00-init-expert",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
