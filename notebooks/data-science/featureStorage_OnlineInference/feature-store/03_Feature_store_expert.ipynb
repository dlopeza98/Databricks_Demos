{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fa93490-df5d-4217-b53c-0b7dd3c89d59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Feature store - full example for Travel recommendation\n",
    "\n",
    "This notebook will illustrate the full capabilities of the feature store to provide recommendation for a Travel website and increase or conversion rate.\n",
    "\n",
    "If you don't know about feature store yet, we recommand you start with the first version to cover the basics.\n",
    "\n",
    "We'll go in details and introduce:\n",
    "\n",
    "* Streaming feature store tables, to refresh your data in near realtime\n",
    "* Live feature computation, reusing the same code for training and inference with the Pandas On Spark APIs (current booking time & distance to location)\n",
    "* Point in time lookup with multiple feature table\n",
    "* Automl to bootstrap model creation\n",
    "\n",
    "In addition, we'll see how we can perform realtime inference:\n",
    "\n",
    "* Create online backed for the feature store table\n",
    "* Create online functions to add additional, realtime feature (distance and date)\n",
    "* Deploy the model using realtime serverless Model Serving fetching features in the online store\n",
    "* Send realtime REST queries for live inference.\n",
    "\n",
    "*Note: For more detail on this notebook, you can read the [Databricks blog post](https://www.databricks.com/blog/best-practices-realtime-feature-computation-databricks) .*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5de7c066-9318-4400-9661-596913156955",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": null
    }
   },
   "source": [
    "### A cluster has been created for this demo\n",
    "To run this demo, just select the cluster `dbdemos-feature-store-david_lopez` from the dropdown menu ([open cluster configuration](https://adb-1126292079753158.18.azuredatabricks.net/#setting/clusters/1220-210214-dtulyc2u/configuration)). <br />\n",
    "*Note: If the cluster was deleted after 30 days, you can re-create it with `dbdemos.create_cluster('feature-store')` or re-install the demo: `dbdemos.install('feature-store')`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ef3dfaf-77f3-41c9-87be-908c1f403365",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-feature-engineering==0.2.0 databricks-sdk==0.20.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c2c9a1e-4989-4aaf-b740-5ff9be7d53d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./_resources/00-init-expert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f06a578-912b-4be9-8b43-e94012da6a46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## New Streaming dataset\n",
    "\n",
    "For this full example demo, we will add a Streaming dataset containing the destination availability that we'll update live. We'll use Spark Streming to ingest the data live. \n",
    "\n",
    "*Note: the streaming flow would typically connect to a message queue like kafka. For this demo we'll consider that our data is updated as file in a blob storage* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2db8b8d2-512e-464a-823d-5cfd000af824",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# 1: Create the feature tables\n",
    "\n",
    "The first step is to create our feature store tables. We'add a new datasource that we'll consume in streaming, making sure our Feature Table is refreshed in near realtime.\n",
    "\n",
    "In addition, we'll compute the \"on-demande\" feature (distance between the user and a destination, booking time) using the pandas API during training, this will allow us to use the same code for realtime inferences.\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/feature_store/feature-store-expert-flow-training.png?raw=true\" width=\"1200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ac6e68e-8083-4d10-a8f1-d2ff2802060e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Compute batch features\n",
    "\n",
    "Calculate the aggregated features from the vacation purchase logs for destination and users. The destination features include popularity features such as impressions, clicks, and pricing features like price at the time of booking. The user features capture the user profile information such as past purchased price. Because the booking data does not change very often, it can be computed once per day in batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b293b726-21a9-4304-8edf-90e62e4ddf53",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Review our silver data"
    }
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM travel_purchase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ba5388a-54ad-469e-8889-d4fa1a2fa2bf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Destination metadata"
    }
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM destination_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e135650-8590-4559-9a70-6023040de025",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Compute features & create our 3 Feature Table"
    }
   },
   "outputs": [],
   "source": [
    "#Delete potential existing tables to reset all the demo\n",
    "delete_fss(catalog, db, [\"user_features\", \"destination_features\", \"destination_location_features\", \"availability_features\"])\n",
    "\n",
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "# Reuse the same features as the previous example 02_Feature_store_advanced\n",
    "# For more details these functions are available under ./_resources/00-init-expert\n",
    "user_features_df = create_user_features(spark.table('travel_purchase'))\n",
    "fe.create_table(name=f\"{catalog}.{db}.user_features\",\n",
    "                primary_keys=[\"user_id\", \"ts\"], \n",
    "                timestamp_keys=\"ts\", \n",
    "                df=user_features_df, \n",
    "                description=\"User Features\")\n",
    "\n",
    "destination_features_df = destination_features_fn(spark.table('travel_purchase'))\n",
    "fe.create_table(name=f\"{catalog}.{db}.destination_features\", \n",
    "                primary_keys=[\"destination_id\", \"ts\"], \n",
    "                timestamp_keys=\"ts\", \n",
    "                df=destination_features_df, \n",
    "                description=\"Destination Popularity Features\")\n",
    "\n",
    "\n",
    "#Add the destination location dataset\n",
    "destination_location = spark.table(\"destination_location\")\n",
    "fe.create_table(name=f\"{catalog}.{db}.destination_location_features\", \n",
    "                primary_keys=\"destination_id\", \n",
    "                df=destination_location, \n",
    "                description=\"Destination location features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67835aa7-06b7-464b-9c73-9a635da0c2e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Compute streaming features\n",
    "\n",
    "Availability of the destination can hugely affect the prices. Availability can change frequently especially around the holidays or long weekends during busy season. This data has a freshness requirement of every few minutes, so we use Spark structured streaming to ensure data is fresh when doing model prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af3c2332-00e7-4f43-9bdf-83e206a23abf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql('CREATE VOLUME IF NOT EXISTS feature_store_volume')\n",
    "destination_availability_stream = (\n",
    "  spark.readStream\n",
    "  .format(\"cloudFiles\")\n",
    "  .option(\"cloudFiles.format\", \"json\") #Could be \"kafka\" to consume from a message queue\n",
    "  .option(\"cloudFiles.inferSchema\", \"true\")\n",
    "  .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "  .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n",
    "  .option(\"cloudFiles.schemaHints\", \"event_ts timestamp, booking_date date, destination_id int\")\n",
    "  .option(\"cloudFiles.schemaLocation\", f\"/Volumes/{catalog}/{db}/feature_store_volume/stream/availability_schema\")\n",
    "  .option(\"cloudFiles.maxFilesPerTrigger\", 100) #Simulate streaming\n",
    "  .load(\"/databricks-datasets/travel_recommendations_realtime/raw_travel_data/fs-demo_destination-availability_logs/json\")\n",
    "  .drop(\"_rescued_data\")\n",
    "  .withColumnRenamed(\"event_ts\", \"ts\")\n",
    ")\n",
    "\n",
    "DBDemos.stop_all_streams_asynch(sleep_time=30)\n",
    "display(destination_availability_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41c5b59e-37b3-49d8-9f7d-2b0ab5dc6c89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fe.create_table(\n",
    "    name=f\"{catalog}.{db}.availability_features\", \n",
    "    primary_keys=[\"destination_id\", \"booking_date\", \"ts\"],\n",
    "    timestamp_keys=[\"ts\"],\n",
    "    schema=destination_availability_stream.schema,\n",
    "    description=\"Destination Availability Features\"\n",
    ")\n",
    "\n",
    "# Now write the data to the feature table in \"merge\" mode using a stream\n",
    "fe.write_table(\n",
    "    name=f\"{catalog}.{db}.availability_features\", \n",
    "    df=destination_availability_stream,\n",
    "    mode=\"merge\",\n",
    "    checkpoint_location= f\"/Volumes/{catalog}/{db}/feature_store_volume/stream/availability_checkpoint\",\n",
    "    trigger={'once': True} #Refresh the feature store table once, or {'processingTime': '1 minute'} for every minute-\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0ecca05-1b73-4078-9f1d-32570fd72f4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Compute on-demand live features\n",
    "\n",
    "User location is a context feature that is captured at the time of the query. This data is not known in advance. \n",
    "\n",
    "Derivated features can be computed from this location. For example, user distance from destination can only be computed in realtime at the prediction time.\n",
    "\n",
    "This introduce a new challenge, we now have to link some function to transform the data and make sure the same is being used for training and inference (batch or realtime). \n",
    "\n",
    "To solve this, Databricks introduced Feature Spec. With Feature Spec, you can create custom function (SQL/PYTHON) to transform your data into new features, and link them to your model and feature store.\n",
    "\n",
    "Because it's shipped as part of your FeatureLookup definition, the same code will be used at inference time, offering a garantee that we compute the feature the same way, and adding flexibility while increasing model version.\n",
    "\n",
    "Note that this function will be available as `catalog.schema.distance_udf` in the browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fd2e310-0059-444b-bdcb-4e97a5c7e9a5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define a function to compute distance "
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION distance_udf(lat1 DOUBLE, lon1 DOUBLE, lat2 DOUBLE, lon2 DOUBLE)\n",
    "RETURNS DOUBLE\n",
    "LANGUAGE PYTHON\n",
    "COMMENT 'Calculate hearth distance from latitude and longitude'\n",
    "AS $$\n",
    "  import numpy as np\n",
    "  dlat, dlon = np.radians(lat2 - lat1), np.radians(lon2 - lon1)\n",
    "  a = np.sin(dlat/2)**2 + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.sin(dlon/2)**2\n",
    "  return 2 * 6371 * np.arcsin(np.sqrt(a))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75e8f7d6-bf82-4958-bd02-032fdd0ba80a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Try the function to compute the distance between a user and a destination"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT distance_udf(user_latitude, user_longitude, latitude, longitude) AS hearth_distance, *\n",
    "    FROM destination_location_features\n",
    "        JOIN destination_features USING (destination_id)\n",
    "        JOIN user_features USING (ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5432ebaf-310e-43cb-a880-df33c6477fba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# 2: Train a custom model with batch, on-demand and streaming features\n",
    "\n",
    "That's all we have to do. We're now ready to train our model with this new feature.\n",
    "\n",
    "*Note: In a typical deployment, you would add more functions such as timestamp features (cos/sin for the hour/day of the week) etc.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ad03011-9ba3-416e-b02f-1aa51a1a6e2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Get ground-truth training labels and key + timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2a2b83b-9cfe-4a46-8ad8-17dbece570e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split to define a training and inference set\n",
    "training_keys = spark.table('travel_purchase').select('ts', 'purchased', 'destination_id', 'user_id', 'user_latitude', 'user_longitude', 'booking_date')\n",
    "training_df = training_keys.where(\"ts < '2022-11-23'\")\n",
    "test_df = training_keys.where(\"ts >= '2022-11-23'\").cache()\n",
    "\n",
    "display(training_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ba89994-e981-4803-aaf8-bd7e5371250b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Create the training set\n",
    "\n",
    "Note the use of `FeatureFunction`, pointing to the new distance_udf function that we saved in Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5573afa6-dd54-4563-816f-6d522f99e63a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define Feature Lookups (for batch and streaming input features)"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "from databricks.feature_engineering.entities.feature_function import FeatureFunction\n",
    "from databricks.feature_engineering.entities.feature_lookup import FeatureLookup\n",
    "\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "feature_lookups = [ # Grab all useful features from different feature store tables\n",
    "  FeatureLookup(\n",
    "      table_name=\"user_features\", \n",
    "      lookup_key=\"user_id\",\n",
    "      timestamp_lookup_key=\"ts\",\n",
    "      feature_names=[\"mean_price_7d\"]\n",
    "  ),\n",
    "  FeatureLookup(\n",
    "      table_name=\"destination_features\", \n",
    "      lookup_key=\"destination_id\",\n",
    "      timestamp_lookup_key=\"ts\"\n",
    "  ),\n",
    "  FeatureLookup(\n",
    "      table_name=\"destination_location_features\",  \n",
    "      lookup_key=\"destination_id\",\n",
    "      feature_names=[\"latitude\", \"longitude\"]\n",
    "  ),\n",
    "  FeatureLookup(\n",
    "      table_name=\"availability_features\", \n",
    "      lookup_key=[\"destination_id\", \"booking_date\"],\n",
    "      timestamp_lookup_key=\"ts\",\n",
    "      feature_names=[\"availability\"]\n",
    "  ),\n",
    "  # Add our function to compute the distance between the user and the destination \n",
    "  FeatureFunction(\n",
    "      udf_name=\"distance_udf\",\n",
    "      input_bindings={\"lat1\": \"user_latitude\", \"lon1\": \"user_longitude\", \"lat2\": \"latitude\", \"lon2\": \"longitude\"},\n",
    "      output_name=\"distance\"\n",
    "  )]\n",
    "\n",
    "#Create the training set\n",
    "training_set = fe.create_training_set(\n",
    "    df=training_df,\n",
    "    feature_lookups=feature_lookups,\n",
    "    exclude_columns=['user_id', 'destination_id', 'booking_date', 'clicked', 'price'],\n",
    "    label='purchased'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66a5e13c-7ef0-4327-96dc-6966bd3ffb69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "training_set_df = training_set.load_df()\n",
    "#Let's cache the training dataset for automl (to avoid recomputing it everytime)\n",
    "training_features_df = training_set_df.cache()\n",
    "\n",
    "display(training_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1776024e-7269-4125-8266-02d5431ed900",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Use automl to build an ML model out of the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "accbc104-8f7d-4edf-bf95-a0d25a3bacf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from databricks import automl\n",
    "xp_path = \"/Shared/dbdemos/experiments/feature-store\"\n",
    "xp_name = f\"automl_purchase_expert_{datetime.now().strftime('%Y-%m-%d_%H:%M:%S')}\"\n",
    "summary_cl = automl.classify(\n",
    "    experiment_name = xp_name,\n",
    "    experiment_dir = xp_path,\n",
    "    dataset = training_features_df,\n",
    "    target_col = \"purchased\",\n",
    "    primary_metric=\"log_loss\",\n",
    "    timeout_minutes = 15\n",
    ")\n",
    "#Make sure all users can access dbdemos shared experiment\n",
    "DBDemos.set_experiment_permission(f\"{xp_path}/{xp_name}\")\n",
    "\n",
    "print(f\"Best run id: {summary_cl.best_trial.mlflow_run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0021cafb-9ba4-4d2e-b39c-02afc640a0b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Save our best model to MLflow registry\n",
    "\n",
    "Next, we'll get Automl best model and add it to our registry. Because we the feature store to keep track of our model & features, we'll log the best model as a new run using the `FeatureStoreClient.log_model()` function.\n",
    "\n",
    "Because our model need live features, we'll wrap our best model with `OnDemandComputationModelWrapper`\n",
    "\n",
    "Because we re-define the `.predict()` function, the wrapper will automatically add our live feature depending on the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f1368f8-8b1f-411d-b935-70509afa7c33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"dbdemos_fs_travel_model_expert\"\n",
    "model_full_name = f\"{catalog}.{db}.{model_name}\"\n",
    "\n",
    "mlflow.set_registry_uri('databricks-uc')\n",
    "# creating sample input to be logged (do not include the live features in the schema as they'll be computed within the model)\n",
    "df_sample = training_set_df.limit(10).toPandas()\n",
    "x_sample = df_sample.drop(columns=[\"purchased\"])\n",
    "dataset = mlflow.data.from_pandas(x_sample)\n",
    "\n",
    "# getting the model created by AutoML \n",
    "best_model = summary_cl.best_trial.load_model()\n",
    "\n",
    "#Get the conda env from automl run\n",
    "artifacts_path = mlflow.artifacts.download_artifacts(run_id=summary_cl.best_trial.mlflow_run_id)\n",
    "env = mlflow.pyfunc.get_default_conda_env()\n",
    "with open(artifacts_path+\"model/requirements.txt\", 'r') as f:\n",
    "    env['dependencies'][-1]['pip'] = f.read().split('\\n')\n",
    "\n",
    "#Create a new run in the same experiment as our automl run.\n",
    "with mlflow.start_run(run_name=\"best_fs_model_expert\", experiment_id=summary_cl.experiment.experiment_id) as run:\n",
    "  #Use the feature store client to log our best model\n",
    "  mlflow.log_input(dataset, \"training\")\n",
    "  fe.log_model(\n",
    "              model=best_model, # object of your model\n",
    "              artifact_path=\"model\", #name of the Artifact under MlFlow\n",
    "              flavor=mlflow.sklearn, # flavour of the model (our LightGBM model has a SkLearn Flavour)\n",
    "              training_set=training_set, # training set you used to train your model with AutoML\n",
    "              input_example=x_sample, # Dataset example (Pandas dataframe)\n",
    "              registered_model_name=model_full_name, # register your best model\n",
    "              conda_env=env)\n",
    "\n",
    "  #Copy automl images & params to our FS run\n",
    "  for item in os.listdir(artifacts_path):\n",
    "    if item.endswith(\".png\"):\n",
    "      mlflow.log_artifact(artifacts_path+item)\n",
    "  mlflow.log_metrics(summary_cl.best_trial.metrics)\n",
    "  mlflow.log_params(summary_cl.best_trial.params)\n",
    "  mlflow.log_param(\"automl_run_id\", summary_cl.best_trial.mlflow_run_id)\n",
    "  mlflow.set_tag(key='feature_store', value='expert_demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c733b0a8-718a-4ad0-8168-07ee1d345932",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Save best model in the registry & flag it as Production ready"
    }
   },
   "outputs": [],
   "source": [
    "latest_model = get_last_model_version(model_full_name)\n",
    "#Move it in Production\n",
    "production_alias = \"production\"\n",
    "if len(latest_model.aliases) == 0 or latest_model.aliases[0] != production_alias:\n",
    "  print(f\"updating model {latest_model.version} to Production\")\n",
    "  mlflow_client = MlflowClient(registry_uri=\"databricks-uc\")\n",
    "  mlflow_client.set_registered_model_alias(model_full_name, production_alias, version=latest_model.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8b25cf2-06d9-444b-9651-d437e2bb7887",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Our model is ready! you can open the Unity Catalog Explorer to review it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a086b76-9085-42f4-8f60-9221ef982d48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Batch score test set\n",
    "\n",
    "Let's make sure our model is working as expected and try to score our test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10e7b07b-c462-48ae-ab58-2cf2c97927e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scored_df = fe.score_batch(model_uri=f\"models:/{model_full_name}@{production_alias}\", df=test_df, result_type=\"boolean\")\n",
    "display(scored_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82de4449-f7dc-4ba1-a475-ce2f43375a8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca5a1924-ff35-4595-aaf0-b1260bd71c54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# simply convert the original probability predictions to true or false\n",
    "pd_scoring = scored_df.select(\"purchased\", \"prediction\").toPandas()\n",
    "print(\"Accuracy: \", accuracy_score(pd_scoring[\"purchased\"], pd_scoring[\"prediction\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7b74642-8c2a-48f0-aaba-aca648c020f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3: Real time serving and inference\n",
    "\n",
    "We're now going to deploy our model supporting real time inference.\n",
    "\n",
    "To provide inference with ms response time, we need to be able to lookup the features for a single user or destination with low latencies.\n",
    "\n",
    "To do that, we'll deploy online stores. These are fully serverless and managed by Databricks. You can think of them as a  (K/V store, such as Mysql, dynamoDB, CosmoDB...).\n",
    "\n",
    "Databricks will automatically synchronize the Delta Live Table content with the online store (you can chose to trigger the update yourself or do it on a schedule).\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/feature_store/feature-store-expert-flow.png?raw=true\" width=\"1200px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c80fb43-0d24-4432-b731-9db5c33f7f45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Publish feature tables as Databricks-managed online tables\n",
    "\n",
    "By publishing our tables to a Databricks-managed online table, Databricks will automatically synchronize the data written to your feature store to the realtime backend.\n",
    "\n",
    "Apart from Databricks-managed online tables, Databricks also supports different third-party backends. You can find more information about integrating Databricks feature tables with third-party online stores in the links below.\n",
    "\n",
    "* AWS dynamoDB ([doc](https://docs.databricks.com/machine-learning/feature-store/online-feature-stores.html))\n",
    "* Azure cosmosDB [doc](https://learn.microsoft.com/en-us/azure/databricks/machine-learning/feature-store/online-feature-stores)\n",
    "\n",
    "\n",
    "**Important note for Azure users:** please make sure you have installed [Azure Cosmos DB Apache Spark 3 OLTP Connector for API for NoSQL](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/sdk-java-spark-v3) (i.e. `com.azure.cosmos.spark:azure-cosmos-spark_3-2_2-12:4.17.2`) to your cluster before running this demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e0413ed-06e2-4ad3-8319-ce65216ef702",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Publish the feature store with online table specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5750af47-85f5-4313-ad96-ad569fdaca2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "def create_online_table(table_name, pks, timeseries_key=None):\n",
    "    w = WorkspaceClient()\n",
    "    online_table_name = table_name+\"_online\"\n",
    "    if not online_table_exists(online_table_name):\n",
    "        from databricks.sdk.service import catalog as c\n",
    "        print(f\"Creating online table for {online_table_name}...\")\n",
    "        spark.sql(f'ALTER TABLE {table_name} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)')\n",
    "        spec = c.OnlineTableSpec(source_table_full_name=table_name, primary_key_columns=pks, run_triggered={'triggered': 'true'}, timeseries_key=timeseries_key)\n",
    "        w.online_tables.create(name=online_table_name, spec=spec)\n",
    "\n",
    "create_online_table(f\"{catalog}.{db}.user_features\",                 [\"user_id\"], \"ts\")\n",
    "create_online_table(f\"{catalog}.{db}.destination_features\",          [\"destination_id\"], \"ts\")\n",
    "create_online_table(f\"{catalog}.{db}.destination_location_features\", [\"destination_id\"])\n",
    "create_online_table(f\"{catalog}.{db}.availability_features\",         [\"destination_id\", \"booking_date\"], \"ts\")\n",
    "\n",
    "#wait for all the tables to be online\n",
    "wait_for_online_tables(catalog, db, [\"user_features_online\", \"destination_features_online\", \"destination_location_features_online\", \"availability_features_online\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edb67bb7-b077-41f3-aea6-4e761fec5a0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Deploy Serverless Model serving Endpoint\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/feature_store/feature-store-expert-model-serving.png?raw=true\" style=\"float: right\" width=\"500px\">\n",
    "\n",
    "Once our Model, Function and Online feature store are in Unity Catalog, we can deploy the model as using Databricks Model Serving.\n",
    "\n",
    "This will provide a REST API to serve our model in realtime.\n",
    "\n",
    "### Enable model inference via the UI\n",
    "\n",
    "After calling `log_model`, a new version of the model is saved. To provision a serving endpoint, follow the steps below.\n",
    "\n",
    "1. Within the Machine Learning menu, click [Serving menu](ml/endpoints) in the left sidebar. \n",
    "2. Create a new endpoint, select the most recent model version from Unity Catalog and start the serverless model serving\n",
    "\n",
    "You can use the UI, in this demo We will use the API to programatically start the endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cb14344-ad73-47e5-932a-005cad4013e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "endpoint_name = \"dbdemos_feature_store_endpoint_expert\"\n",
    "wc = WorkspaceClient()\n",
    "served_models =[ServedModelInput(model_full_name, model_version=latest_model.version, workload_size=ServedModelInputWorkloadSize.SMALL, scale_to_zero_enabled=True)]\n",
    "try:\n",
    "    print(f'Creating endpoint {endpoint_name} with latest version...')\n",
    "    wc.serving_endpoints.create_and_wait(endpoint_name, config=EndpointCoreConfigInput(served_models=served_models))\n",
    "except Exception as e:\n",
    "    if 'already exists' in str(e):\n",
    "        print(f'Endpoint exists, updating with latest model version...')\n",
    "        wc.serving_endpoints.update_config_and_wait(endpoint_name, served_models=served_models)\n",
    "    else: \n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2607edf5-8b44-42bb-adaf-ec2bfe660c74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/feature_store/feature-store-expert-model-serving-inference.png?raw=true\" style=\"float: right\" width=\"700\"/>\n",
    "\n",
    "Once our model deployed, you can easily test your model using the Model Serving endpoint UI.\n",
    "\n",
    "Let's call it using the REST API directly.\n",
    "\n",
    "The endpoint will answer in millisec, what will happen under the hood is the following:\n",
    "\n",
    "* The endpoint receive the REST api call\n",
    "* It calls our 4 online table to get the features\n",
    "* Call the `distance_udf` function to compute the distance\n",
    "* Call the ML model\n",
    "* Returns the final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cd10677-20ba-4b16-9966-5e249eb048dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "lookup_keys = test_df.drop('purchased').limit(2).toPandas().astype({'ts': 'str', 'booking_date': 'str'}).to_dict(orient=\"records\")\n",
    "print(f'Compute the propensity score for these customers: {lookup_keys}')\n",
    "#Query the endpoint\n",
    "for i in range(3):\n",
    "    starting_time = timeit.default_timer()\n",
    "    inferences = wc.serving_endpoints.query(endpoint_name, inputs=lookup_keys)\n",
    "    print(f\"Inference time, end 2 end :{round((timeit.default_timer() - starting_time)*1000)}ms\")\n",
    "    print(inferences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea1d82bb-29e3-4de8-9f66-1e92a8a7bca0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Optional: Deploy our Function as Feature Spec to compute transformations in realtime\n",
    "\n",
    "Our function can be saved as a Feature Spec and deployed standalone Model Serving Endpoint to serve any transformation.\n",
    "\n",
    "Here is an example on how you can create a feature spec to compute the distance between 2 points.\n",
    "\n",
    "This feature spec will do the lookup for you and call the `distance_df` function (as define in the `feature_lookups`).\n",
    "\n",
    "Once the feature spec deployed, you can use the [Serving Endpoint menu](ml/endpoints) to create a new endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29452c48-bfe0-4b54-9c7d-33d3d2ac86bb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Save the feature spec within Unity Catalog"
    }
   },
   "outputs": [],
   "source": [
    "feature_spec_name = f\"{catalog}.{db}.travel_feature_spec\"\n",
    "try:\n",
    "    fe.create_feature_spec(name=feature_spec_name, features=feature_lookups, exclude_columns=['user_id', 'destination_id', 'booking_date', 'clicked', 'price'])\n",
    "except Exception as e:\n",
    "    if \"RESOURCE_ALREADY_EXISTS\" not in str(e): raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa2bce10-ac49-4b93-b901-fcd9bab72617",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create the endpoint using the API"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering.entities.feature_serving_endpoint import AutoCaptureConfig, EndpointCoreConfig, ServedEntity\n",
    "\n",
    "# Create endpoint\n",
    "feature_endpoint_name = \"dbdemos-fse-travel-spec\"\n",
    "try: \n",
    "    status = fe.create_feature_serving_endpoint(name=feature_endpoint_name, \n",
    "                                                config=EndpointCoreConfig(served_entities=ServedEntity(scale_to_zero_enabled= True, feature_spec_name=feature_spec_name)))\n",
    "except Exception as e:\n",
    "    if \"already exists\" not in str(e): raise e\n",
    "\n",
    "ep = wait_for_feature_endpoint_to_start(fe, feature_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b552e8fb-bff8-457c-91d8-54f700751b7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's try our new feature spec endpoints. We can send send our queries using the UI or via REST API directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "727d0fb5-385d-4fc8-b3e2-d6de2e46c85f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f'Compute the propensity score for these customers: {lookup_keys}')\n",
    "\n",
    "def query_endpoint(url, lookup_keys):\n",
    "    return requests.request(method='POST', headers=get_headers(), url=url, json={'dataframe_records': lookup_keys}).json()\n",
    "query_endpoint(ep.url+\"/invocations\", lookup_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "419661f2-00fb-43de-b388-bb74828d2940",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Conclusion\n",
    "\n",
    "In this series of demos you've learned how to use **Databricks Feature Store** in 3 different manner:\n",
    "- `batch (offline Feature Store)`\n",
    "- `streaming (offline Feature Store)`\n",
    "- `real-time (online Feature Store)`\n",
    "\n",
    "The use of the each from the above would depend whether your organization requires scheduled batch jobs, near real-time streaming or real-time on the fly computations. \n",
    "\n",
    "To summarize, if you required to have a real-time feature computations, then figure out what type of data you have, data freshness and latency requirements and make sure to:\n",
    "\n",
    "- Map your data to batch, streaming, and on-demand computational architecture based on data freshness requirements.\n",
    "- Use spark structured streaming to stream the computation to offline store and online store\n",
    "- Use on-demand computation with MLflow pyfunc\n",
    "- Use Databricks Serverless realtime inference to perform low-latency predictions on your model"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "03_Feature_store_expert",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
