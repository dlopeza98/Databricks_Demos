{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22fd2d6b-c689-4f87-82a0-1fb2d1814538",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Feature store Travel Agency recommendation - Advanced\n",
    "\n",
    "For this demo, we'll go deeper in the Feature Store capabilities, adding multiple Feature Store table and introducing point in time lookup.\n",
    "\n",
    "We'll use the same dataset as before and implement the same use-case: recommender model for a Travel agency, pushing personalized offer based on what our customers are the most likely to buy.\n",
    "\n",
    "**What you will learn:**\n",
    "- Build more advanced features with multiple tables\n",
    "- Introduce timestamp key, to simplify temporal feature management\n",
    "- Use AutoML to create the best model for us\n",
    "- Online table, synchronizing Databricks Delta Table with a real time, low latency table automatically used by your model to lookup features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc4ed200-c62c-4af7-a549-d121fb7390e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": null
    }
   },
   "source": [
    "### A cluster has been created for this demo\n",
    "To run this demo, just select the cluster `dbdemos-feature-store-david_lopez` from the dropdown menu ([open cluster configuration](https://adb-1126292079753158.18.azuredatabricks.net/#setting/clusters/1220-210214-dtulyc2u/configuration)). <br />\n",
    "*Note: If the cluster was deleted after 30 days, you can re-create it with `dbdemos.create_cluster('feature-store')` or re-install the demo: `dbdemos.install('feature-store')`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b7e914c-5802-4b47-a79a-b4b91045fae4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-feature-engineering==0.2.0 databricks-sdk==0.20.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7de2cb4e-e770-448d-b97e-231a5d4c68b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./_resources/00-init-basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8cbe168-9132-4100-b953-6b378debe184",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Review our silver data"
    }
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM travel_purchase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7ea2268-327e-4142-9134-38d26b6c0213",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 1: Create our Feature Tables\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/feature_store/feature-store-advanced-flow.png?raw=true\" width=\"800px\" style=\"float: right\">\n",
    "\n",
    "In this second example, we'll introduce more tables and new features calculated with window functions.\n",
    "\n",
    "To simplify updates & refresh, we'll split them in 2 tables:\n",
    "\n",
    "* **User features**: contains all the features for a given user in a given point in time (location, previous purchases if any etc)\n",
    "* **Destination features**: data on the travel destination for a given point in time (interest tracked by the number of clicks & impression)\n",
    "\n",
    "### Point-in-time support for feature tables\n",
    "\n",
    "Databricks Feature Store supports use cases that require point-in-time correctness.\n",
    "\n",
    "The data used to train a model often has time dependencies built into it. In our case, because we are adding rolling-window features, our Feature Table will contain data on all the dataset timeframe. \n",
    "\n",
    "When we build our model, we must consider only feature values up until the time of the observed target value. If you do not explicitly take into account the timestamp of each observation, you might inadvertently use feature values measured after the timestamp of the target value for training. This is called “data leakage” and can negatively affect the model’s performance.\n",
    "\n",
    "Time series feature tables include a timestamp key column that ensures that each row in the training dataset represents the latest known feature values as of the row’s timestamp. \n",
    "\n",
    "In our case, this timestamp key will be the `ts` field, present in our 2 feature tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e831e1f-d47a-4d5d-94dc-bac7f72ffa8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Calculating the features\n",
    "\n",
    "Let's calculate the aggregated features from the vacation purchase logs for destinations and users. \n",
    "\n",
    "The user features capture the user profile information such as past purchased price. Because the booking data does not change very often, it can be computed once per day in batch.\n",
    "\n",
    "The destination features include popularity features such as impressions and clicks, as well as pricing features such as price at the time of booking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc2f392d-73c7-4bef-bde0-6f9ceb2f00e4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "User Features"
    }
   },
   "outputs": [],
   "source": [
    "def create_user_features(travel_purchase_df):\n",
    "    \"\"\"\n",
    "    Computes the user_features feature group.\n",
    "    \"\"\"\n",
    "    travel_purchase_df = travel_purchase_df.withColumn('ts_l', F.col(\"ts\").cast(\"long\"))\n",
    "    travel_purchase_df = (\n",
    "        # Sum total purchased for 7 days\n",
    "        travel_purchase_df.withColumn(\"lookedup_price_7d_rolling_sum\",\n",
    "            F.sum(\"price\").over(w.Window.partitionBy(\"user_id\").orderBy(F.col(\"ts_l\")).rangeBetween(start=-(7 * 86400), end=0))\n",
    "        )\n",
    "        # counting number of purchases per week\n",
    "        .withColumn(\"lookups_7d_rolling_sum\", \n",
    "            F.count(\"*\").over(w.Window.partitionBy(\"user_id\").orderBy(F.col(\"ts_l\")).rangeBetween(start=-(7 * 86400), end=0))\n",
    "        )\n",
    "        # total price 7d / total purchases for 7 d \n",
    "        .withColumn(\"mean_price_7d\",  F.col(\"lookedup_price_7d_rolling_sum\") / F.col(\"lookups_7d_rolling_sum\"))\n",
    "         # converting True / False into 1/0\n",
    "        .withColumn(\"tickets_purchased\", F.col(\"purchased\").cast('int'))\n",
    "        # how many purchases for the past 6m\n",
    "        .withColumn(\"last_6m_purchases\", \n",
    "            F.sum(\"tickets_purchased\").over(w.Window.partitionBy(\"user_id\").orderBy(F.col(\"ts_l\")).rangeBetween(start=-(6 * 30 * 86400), end=0)).cast('double')\n",
    "        )\n",
    "        .select(\"user_id\", \"ts\", \"mean_price_7d\", \"last_6m_purchases\", \"user_longitude\", \"user_latitude\")\n",
    "    )\n",
    "    return add_time_features_spark(travel_purchase_df)\n",
    "\n",
    "\n",
    "user_features_df = create_user_features(spark.table('travel_purchase'))\n",
    "display(user_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8afc04bf-118b-4a90-8df1-71ed0d111a7b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Destination Features"
    }
   },
   "outputs": [],
   "source": [
    "def create_destination_features(travel_purchase_df):\n",
    "    \"\"\"\n",
    "    Computes the destination_features feature group.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        travel_purchase_df\n",
    "          .withColumn(\"clicked\", F.col(\"clicked\").cast(\"int\"))\n",
    "          .withColumn(\"sum_clicks_7d\", \n",
    "            F.sum(\"clicked\").over(w.Window.partitionBy(\"destination_id\").orderBy(F.col(\"ts\").cast(\"long\")).rangeBetween(start=-(7 * 86400), end=0))\n",
    "          )\n",
    "          .withColumn(\"sum_impressions_7d\", \n",
    "            F.count(\"*\").over(w.Window.partitionBy(\"destination_id\").orderBy(F.col(\"ts\").cast(\"long\")).rangeBetween(start=-(7 * 86400), end=0))\n",
    "          )\n",
    "          .select(\"destination_id\", \"ts\", \"sum_clicks_7d\", \"sum_impressions_7d\")\n",
    "    )  \n",
    "destination_features_df = create_destination_features(spark.table('travel_purchase'))\n",
    "display(destination_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2e84d5c-d707-4862-8fb5-265f8c442ce1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Creating the Feature Table\n",
    "\n",
    "Let's use the FeatureStore client to save our 2 tables. Note the `timestamp_keys='ts'` parameters that we're adding during the table creation.\n",
    "\n",
    "Databricks Feature Store will use this information to automatically filter features and prevent from potential leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67eb7431-0416-4225-b8b5-39841a29a613",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "user_features"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "fe = FeatureEngineeringClient(model_registry_uri=\"databricks-uc\")\n",
    "# help(fe.create_table)\n",
    "\n",
    "# first create a table with User Features calculated above \n",
    "fe_table_name_users = f\"{catalog}.{db}.user_features_advanced\"\n",
    "#fe.drop_table(name=fe_table_name_users)\n",
    "fe.create_table(\n",
    "    name=fe_table_name_users, # unique table name (in case you re-run the notebook multiple times)\n",
    "    primary_keys=[\"user_id\", \"ts\"],\n",
    "    timestamp_keys=\"ts\",\n",
    "    df=user_features_df,\n",
    "    description=\"User Features\",\n",
    "    tags={\"team\":\"analytics\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5eb3dcd-fca4-491a-93e0-a1f3f82a78af",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "destination_features"
    }
   },
   "outputs": [],
   "source": [
    "fe_table_name_destinations = f\"{catalog}.{db}.destination_features_advanced\"\n",
    "# second create another Feature Table from popular Destinations\n",
    "# for the second table, we show how to create and write as two separate operations\n",
    "# fe.drop_table(name=fe_table_name_destinations)\n",
    "fe.create_table(\n",
    "    name=fe_table_name_destinations, # unique table name (in case you re-run the notebook multiple times)\n",
    "    primary_keys=[\"destination_id\", \"ts\"],\n",
    "    timestamp_keys=\"ts\", \n",
    "    schema=destination_features_df.schema,\n",
    "    description=\"Destination Popularity Features\",\n",
    "    tags={\"team\":\"analytics\"} # if you have multiple team creating tables, maybe worse of adding a tag \n",
    ")\n",
    "fe.write_table(name=fe_table_name_destinations, df=destination_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f19cff04-4bbf-406e-9b4e-17b6427f6c58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/feature_store/feature-store-advanced-tables.png?raw=true\" style=\"float: right; margin-left: 10px\" width=\"550px\">\n",
    "\n",
    "As in our previous example, the 2 feature store tables were created and are available within Unity Catalog. \n",
    "\n",
    "You can explore Catalog Explorer. You'll find all the features created, including a reference to this notebook and the version used during the feature table creation.\n",
    "\n",
    "Note that the id ts are automatically defined as PK and TS PK columns.\n",
    "\n",
    "Now that our features are ready, we can start creating the training dataset and train a model using AutoML!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f811d81-dca0-4c50-b818-8973d463f9c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 2: Train a model with FS and timestamp lookup\n",
    "\n",
    "### Create the training dataset\n",
    "\n",
    "The next step is to build a training dataset. \n",
    "\n",
    "Because we have 2 feature tables, we'll add 2 `FeatureLookup` entries, specifying the key so that the feature store engine can join using this field.\n",
    "\n",
    "We will also add the `timestamp_lookup_key` property to `ts` so that the engine filter the features based on this key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dd54d37-077b-44c8-91c6-4e0b85b5dca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ground_truth_df = spark.table('travel_purchase').select('user_id', 'destination_id', 'purchased', 'ts')\n",
    "\n",
    "# Split based on time to define a training and inference set (we'll do train+eval on the past & test in the most current value)\n",
    "training_labels_df = ground_truth_df.where(\"ts < '2022-11-23'\")\n",
    "test_labels_df = ground_truth_df.where(\"ts >= '2022-11-23'\")\n",
    "\n",
    "display(test_labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1575f7ce-a803-4a94-b4b6-70a0c04325ae",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create the feature lookup with the lookup keys"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient, FeatureLookup\n",
    "from databricks.feature_store import feature_table, FeatureLookup\n",
    "\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "model_feature_lookups = [\n",
    "      FeatureLookup(\n",
    "          table_name=fe_table_name_destinations,\n",
    "          lookup_key=\"destination_id\",\n",
    "          timestamp_lookup_key=\"ts\"\n",
    "      ),\n",
    "      FeatureLookup(\n",
    "          table_name=fe_table_name_users,\n",
    "          lookup_key=\"user_id\",\n",
    "          feature_names=[\"mean_price_7d\", \"last_6m_purchases\", \"day_of_week_sin\", \"day_of_week_cos\", \"day_of_month_sin\", \"day_of_month_cos\", \"hour_sin\", \"hour_cos\"], # if you dont specify here the FeatureEngineeringClient will take all your feature apart from primary_keys \n",
    "          timestamp_lookup_key=\"ts\"\n",
    "      )\n",
    "]\n",
    "\n",
    "# fe.create_training_set will look up features in model_feature_lookups with matched key from training_labels_df\n",
    "training_set = fe.create_training_set(\n",
    "    df=training_labels_df, # joining the original Dataset, with our FeatureLookupTable\n",
    "    feature_lookups=model_feature_lookups,\n",
    "    exclude_columns=[\"ts\", \"destination_id\", \"user_id\"], # exclude id columns as we don't want them as feature\n",
    "    label='purchased',\n",
    ")\n",
    "\n",
    "training_pd = training_set.load_df()\n",
    "display(training_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2345701d-2b3b-4678-8e4a-c86670a121e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating the model using Databricks AutoML \n",
    "\n",
    "Instead of creating a basic model like previously, we will use <a href=\"https://docs.databricks.com/machine-learning/automl/index.html#classification\" target=\"_blank\">Databricks AutoML</a> to train our model, using best practices out of the box.\n",
    "\n",
    "While you can do that using the UI directly (+New => AutoML), we'll be using the `databricks.automl` API to have a reproductible flow.\n",
    "\n",
    "After running the previous cell, you will notice two notebooks and an MLflow experiment:\n",
    "\n",
    "* **Data exploration notebook**: we can see a Profiling Report which organizes the input columns and discusses values, frequency and other information\n",
    "* **Best trial notebook**: shows the source code for reproducing the best trial conducted by AutoML\n",
    "* **MLflow experiment**: contains high level information, such as the root artifact location, experiment ID, and experiment tags. The list of trials contains detailed summaries of each trial, such as the notebook and model location, training parameters, and overall metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "740bda45-359c-4115-a1d6-a717d93614c0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Start an AutoML run"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from databricks import automl\n",
    "xp_path = \"/Shared/dbdemos/experiments/feature-store\"\n",
    "xp_name = f\"automl_purchase_advanced_{datetime.now().strftime('%Y-%m-%d_%H:%M:%S')}\"\n",
    "summary_cl = automl.classify(\n",
    "    experiment_name = xp_name,\n",
    "    experiment_dir = xp_path,\n",
    "    dataset = training_pd,\n",
    "    target_col = \"purchased\",\n",
    "    primary_metric=\"log_loss\",\n",
    "    timeout_minutes = 10\n",
    ")\n",
    "#Make sure all users can access dbdemos shared experiment\n",
    "DBDemos.set_experiment_permission(f\"{xp_path}/{xp_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b906fbc-677e-40ce-9235-7fdaafc7e3ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Get best run from automl MLFlow experiment\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/main/images/product/feature_store/automl_experm.png\" alt=\"step12\" width=\"700\" style=\"float: right; margin-left: 10px\" />\n",
    "\n",
    "Open the **MLflow experiment** from the link above and explore your best run.\n",
    "\n",
    "In a real deployment, we would review the notebook generated and potentially improve it using our domain knowledge before deploying it in production.\n",
    "\n",
    "For this Feature Store demo, we'll simply get the best model and deploy it in the registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82ac3a12-f119-4d22-bc49-ec659c6122cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Saving our best model to MLflow registry\n",
    "\n",
    "Next, we'll get Automl best model and add it to our registry. Because we the feature store to keep track of our model & features, we'll log the best model as a new run using the `FeatureStoreClient.log_model()` function.\n",
    "\n",
    "**summary_cl** provides the automl information required to automate the deployment process. We'll use it to select our best run and deploy as Production production.\n",
    "\n",
    "*Note that another way to find the best run would be to use search_runs function from mlflow API, sorting by our accuracy metric.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42b54e82-0c8f-4a71-a474-dc9a840228b9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Save best model in the registry & flag it as Production ready"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"dbdemos_fs_travel_model_advanced\"\n",
    "model_full_name = f\"{catalog}.{db}.{model_name}\"\n",
    "\n",
    "mlflow.set_registry_uri('databricks-uc')\n",
    "# creating sample input to be logged\n",
    "df_sample = training_pd.limit(10).toPandas()\n",
    "x_sample = df_sample.drop(columns=[\"purchased\"])\n",
    "y_sample = df_sample[\"purchased\"]\n",
    "dataset = mlflow.data.from_pandas(x_sample)\n",
    "\n",
    "# getting the model created by AutoML \n",
    "best_model = summary_cl.best_trial.load_model()\n",
    "\n",
    "env = mlflow.pyfunc.get_default_conda_env()\n",
    "with open(mlflow.artifacts.download_artifacts(\"runs:/\"+summary_cl.best_trial.mlflow_run_id+\"/model/requirements.txt\"), 'r') as f:\n",
    "    env['dependencies'][-1]['pip'] = f.read().split('\\n')\n",
    "\n",
    "#Create a new run in the same experiment as our automl run.\n",
    "with mlflow.start_run(run_name=\"best_fs_model_advanced\", experiment_id=summary_cl.experiment.experiment_id) as run:\n",
    "  #Use the feature store client to log our best model\n",
    "  mlflow.log_input(dataset, \"training\")\n",
    "  fe.log_model(\n",
    "              model=best_model, # object of your model\n",
    "              artifact_path=\"model\", #name of the Artifact under MlFlow\n",
    "              flavor=mlflow.sklearn, # flavour of the model (our LightGBM model has a SkLearn Flavour)\n",
    "              training_set=training_set, # training set you used to train your model with AutoML\n",
    "              input_example=x_sample, # example of the dataset, should be Pandas\n",
    "              signature=infer_signature(x_sample, y_sample), # schema of the dataset, not necessary with FS, but nice to have \n",
    "              registered_model_name=model_full_name, # register your best model\n",
    "              conda_env = env\n",
    "          )\n",
    "  mlflow.log_metrics(summary_cl.best_trial.metrics)\n",
    "  mlflow.log_params(summary_cl.best_trial.params)\n",
    "  mlflow.set_tag(key='feature_store', value='advanced_demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b275ed50-c5ae-407f-b478-06047bea6d3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "latest_model = get_last_model_version(model_full_name)\n",
    "#Move it in Production\n",
    "production_alias = \"production\"\n",
    "if len(latest_model.aliases) == 0 or latest_model.aliases[0] != production_alias:\n",
    "  print(f\"updating model {latest_model.version} to Production\")\n",
    "  mlflow_client = MlflowClient(registry_uri=\"databricks-uc\")\n",
    "  mlflow_client.set_registered_model_alias(model_full_name, production_alias, version=latest_model.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3760127f-1642-48cc-bd42-c41af852418e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 3: Running batch inference\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/main/images/product/feature_store/feature_store_inference_advanced.png\" style=\"float: right\" width=\"850px\" />\n",
    "\n",
    "As previously, we can easily leverage the feature store to get our predictions.\n",
    "\n",
    "No need to fetch or recompute the feature, we just need the lookup ids and the feature store will automatically fetch them from the feature store table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1edd7b56-e243-43f6-8075-23d5286b1e14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## For sake of simplicity, we will just predict on the same inference_data_df\n",
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "fe = FeatureEngineeringClient(model_registry_uri=\"databricks-uc\")\n",
    "batch_scoring = test_labels_df.select('user_id', 'destination_id', 'ts', 'purchased')\n",
    "scored_df = fe.score_batch(model_uri=f\"models:/{model_full_name}@{production_alias}\", df=batch_scoring, result_type=\"boolean\")\n",
    "display(scored_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85fa1f7e-dc27-43f2-b286-baa9cdeddf39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 4: Running realtime inferences: introducing Online Tables\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/feature_store/feature-store-advanced-online.png?raw=true\" style=\"float: right\" width=\"850px\" />\n",
    "\n",
    "Databricks now has built-in **online table**, providing realtime Key-Value lookup for your inferences.\n",
    "\n",
    "Online tables are fully managed and serverless. \n",
    "\n",
    "This let you deploy realtime endpoints using these tables to lookup features and compute your prediction in milliseconds.\n",
    "\n",
    "Simply pick a source Detla Table to create your first online table. \n",
    "\n",
    "Databricks will manage the synchronisation between your Delta Live Table and the Online Table for you in the background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "932ca027-f5aa-40b3-a095-86fa7672606b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating the online tables\n",
    "Creating the table is straight forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3967abfe-62f6-47e0-ba0f-cc019f4d07f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "def create_online_table(table_name, pks, timeseries_key=None):\n",
    "    w = WorkspaceClient()\n",
    "    online_table_name = table_name+\"_online\"\n",
    "    if not online_table_exists(online_table_name):\n",
    "        from databricks.sdk.service import catalog as c\n",
    "        print(f\"Creating online table for {online_table_name}...\")\n",
    "        spark.sql(f'ALTER TABLE {table_name} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)')\n",
    "        spec = c.OnlineTableSpec(source_table_full_name=table_name, primary_key_columns=pks, run_triggered={'triggered': 'true'}, timeseries_key=timeseries_key)\n",
    "        w.online_tables.create(name=online_table_name, spec=spec)\n",
    "        \n",
    "#Note that the timeseries key 'ts' is optional. When defined, the online store will return the most recent entry.\n",
    "create_online_table(f\"{catalog}.{db}.destination_features_advanced\", [\"destination_id\"], \"ts\") \n",
    "create_online_table(f\"{catalog}.{db}.user_features_advanced\",        [\"user_id\"], \"ts\")\n",
    "\n",
    "#wait for all the tables to be online\n",
    "wait_for_online_tables(catalog, db, [\"destination_features_advanced_online\", \"user_features_advanced_online\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cee68ddf-441b-4f1e-b708-ae1facab130d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Our online realtime tables are available in the Unity Catalog Explorer, like any other tables!\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/feature_store/feature-store-advanced-online-tables.png?raw=true\" style=\"float: right\" width=\"850px\" />\n",
    "\n",
    "We can see that our online table has been successfully created. \n",
    "\n",
    "Like any other table, it's available within the Unity Catalog explorer, in your catalog -> schema. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99887b0a-d952-4675-bc28-bcc9186fd9b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Let's deploy our realtime model using the online table.\n",
    "\n",
    "Because you used the Feature Store to save the features and the model, Databricks knows that your model has to leverage the online tables once deployed.\n",
    "\n",
    "All we have to do is simply deploy the Model as a Serving Endpoint, and the Online Tables will automatically be leveraged to lookup features in realtime, all managed by Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a1c3a44-5621-489f-a427-d9b3ecc2033c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "endpoint_name = \"dbdemos_feature_store_endpoint_advanced\"\n",
    "wc = WorkspaceClient()\n",
    "served_models =[ServedModelInput(model_full_name, model_version=latest_model.version, workload_size=ServedModelInputWorkloadSize.SMALL, scale_to_zero_enabled=True)]\n",
    "try:\n",
    "    print(f'Creating endpoint {endpoint_name} with latest version...')\n",
    "    wc.serving_endpoints.create_and_wait(endpoint_name, config=EndpointCoreConfigInput(served_models=served_models))\n",
    "except Exception as e:\n",
    "    if 'already exists' in str(e):\n",
    "        print(f'Endpoint exists, updating with latest model version...')\n",
    "        wc.serving_endpoints.update_config_and_wait(endpoint_name, served_models=served_models)\n",
    "    else: \n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a2745e2-1b76-47bb-bba4-728adde75fa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Querying the online tables\n",
    "\n",
    "Let's query the model. Under the hood, the following will happen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92f06404-10cd-4ed7-bcd2-4e8244efa2c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lookup_keys = test_labels_df.drop('purchased', \"ts\").limit(2).toPandas()\n",
    "data = lookup_keys.to_dict(orient=\"records\")\n",
    "print('Data sent to the model:')\n",
    "print(data)\n",
    "\n",
    "starting_time = timeit.default_timer()\n",
    "inferences = wc.serving_endpoints.query(endpoint_name, inputs=lookup_keys.to_dict(orient=\"records\"))\n",
    "print(f\"Inference time, end 2 end :{round((timeit.default_timer() - starting_time)*1000)}ms\")\n",
    "print(inferences.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b988308f-5422-452d-b113-41762dc255fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Summary \n",
    "\n",
    "We've seen how the feature store can handle multiple tables, leverage a more advanced model with Databricks AutoML and use point-in-time lookup when your dataset contains temporal information and you don't want the future information to leak.\n",
    "\n",
    "On top of that, we saw how Databricks Online table can provide realtime capabilities for K/V queries, automatically backed \n",
    "\n",
    "Databricks Feature store brings you a full traceability, knowing which model is using which feature in which notebook/job.\n",
    "\n",
    "It also simplify inferences by always making sure the same features will be used for model training and inference, always querying the same feature table based on your lookup keys.\n",
    "\n",
    "## Next Steps \n",
    "\n",
    "Open the [03_Feature_store_expert notebook]($./03_Feature_store_expert) to explore more Feature Store benefits & capabilities:\n",
    "\n",
    "- Multiple lookup tables\n",
    "- Streaming datasets\n",
    "- On-demand feature computation wrapped with the model with Feature Spec\n",
    "- Online feature store\n",
    "- Real time model serving with Rest Endpoints"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02_Feature_store_advanced",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
