{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8420d8f9-c14c-45cb-8f5f-358f3de745d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Getting started with Feature Engineering in Databricks Unity Catalog\n",
    "\n",
    "The <a href=\"https://docs.databricks.com/en/machine-learning/feature-store/uc/feature-tables-uc.html\" target=\"_blank\">Feature Engineering in Databricks Unity Catalog</a> allows you to create a centralized repository of features. These features can be used to train & call your ML models. By saving features as feature engineering tables in Unity Catalog, you will be able to:\n",
    "\n",
    "- Share features across your organization \n",
    "- Increase discoverability sharing \n",
    "- Ensures that the same feature computation code is used for model training and inference\n",
    "- Enable real-time backend, leveraging your Delta Lake tables for batch training and Key-Value store for realtime inferences\n",
    "\n",
    "## Demo content\n",
    "\n",
    "Multiple version of this demo are available, each version introducing a new concept and capabilities. We recommend following them 1 by 1.\n",
    "\n",
    "### Introduction (this notebook)\n",
    "\n",
    " - Ingest our data and save them as a feature table within Unity Catalog\n",
    " - Create a Feature Lookup with multiple tables\n",
    " - Train your model using the Feature Engineering Client\n",
    " - Register your best model and promote it into Production\n",
    " - Perform batch scoring\n",
    "\n",
    "### Advanced version ([open the notebook]($./02_Feature_store_advanced))\n",
    "\n",
    " - Join multiple Feature Store tables\n",
    " - Point in time lookup\n",
    " - Online tables\n",
    "\n",
    "### Expert version ([open the notebook]($./03_Feature_store_expert))\n",
    " - Streaming Feature Store tables \n",
    " - Feature spec (with functions) saved in UC \n",
    " - Feature spec endpoint to compute inference features in realtime (like distance)\n",
    "\n",
    " \n",
    "*For more detail on the Feature Engineering in Unity Catalog, open <a href=\"https://api-docs.databricks.com/python/feature-engineering/latest\" target=\"_blank\">the documentation</a>.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df7a1559-c741-44af-88a7-eecd30e86745",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": null
    }
   },
   "source": [
    "### A cluster has been created for this demo\n",
    "To run this demo, just select the cluster `dbdemos-feature-store-david_lopez` from the dropdown menu ([open cluster configuration](https://adb-1126292079753158.18.azuredatabricks.net/#setting/clusters/1220-210214-dtulyc2u/configuration)). <br />\n",
    "*Note: If the cluster was deleted after 30 days, you can re-create it with `dbdemos.create_cluster('feature-store')` or re-install the demo: `dbdemos.install('feature-store')`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "878b5bcb-6466-4a6b-8c0b-6e2dcb142f26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Building a propensity score to book travels & hotels\n",
    "\n",
    "Fot this demo, we'll step in the shoes of a Travel Agency offering deals in their website.\n",
    "\n",
    "Our job is to increase our revenue by boosting the amount of purchases, pushing personalized offer based on what our customers are the most likely to buy.\n",
    "\n",
    "In order to personalize offer recommendation in our application, we have have been asked as a Data Scientist to create the TraveRecommendationModel that predicts the probability of purchasing a given travel. \n",
    "\n",
    "For this first version, we'll use a single data source: **Travel Purchased by users**\n",
    "\n",
    "We're going to make a basic single Feature Table that contains all existing features (**`clicked`** or **`price`**) and a few generated one (derivated from the timestamp). \n",
    "\n",
    "We'll then use these features to train our baseline model, and to predict whether a user is likely to purchased a travel on our Website.\n",
    "\n",
    "*Note that the goal is to understand what feature tables are and how they work, we won't focus on the model itself*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f5aadc7-cd73-48ef-a67c-4333344b5c7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-feature-engineering==0.2.0 databricks-sdk==0.20.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c4b7127-e6de-45d7-9c3d-923dc4ae604e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./_resources/00-init-basic $reset_all_data=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94d0d624-b759-4ca9-9668-a52dc4a4c72c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Let's review our silver table we'll use to create our features"
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "SELECT * FROM travel_purchase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97b381f7-c89c-4f05-b228-c4d7f3c4ee90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note that a Data Sciencist would typically start by exploring the data. We could also use the data profiler integrated into Databricks Notebooks to quickly identify if we have missings values or a skew in our data.\n",
    "\n",
    "*We will keep this part simple as we'll focus on feature engineering*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4cf43ff-7cfb-4403-9abb-95c29f6f0076",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Quick data analysis"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "g = sns.PairGrid(spark.table('travel_purchase').sample(0.01).toPandas()[['price', 'user_latitude', 'user_longitude', 'purchased']], diag_sharey=False, hue=\"purchased\")\n",
    "g.map_lower(sns.kdeplot).map_diag(sns.kdeplot, lw=3).map_upper(sns.regplot).add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4606f90-37ff-4f20-b844-6dfd30b3341e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 1: Create our Feature Engineering table\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/main/images/product/feature_store/feature_store_creation.png\" alt=\"Feature Engineering Table Creation\" width=\"500px\" style=\"margin-left: 10px; float: right\"/>\n",
    "\n",
    "Our first step is to create our Feature Engineering table.\n",
    "\n",
    "We will load data from the silver table `travel_purchase` and create features from these values. \n",
    "\n",
    "In this first version, we'll transform the timestamp into multiple features that our model will be able to understand. \n",
    "\n",
    "In addition, we will drop the label from the table as we don't want it to leak our features when we do our training.\n",
    "\n",
    "To create the feature table, we'll use the `FeatureEngineeringClient.create_table`. \n",
    "\n",
    "Under the hood, this will create a Delta Table to save our information. \n",
    "\n",
    "These steps would typically live in a separate job that we call to refresh our features when new data lands in the silver table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b258a8a4-6678-4461-99b3-2792ec2a49b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Compute the features \n",
    "\n",
    "Let's create the features that we'll save in our Feature Table. We'll keep it simple this first example, changing the data type and add extra columns based on the date.\n",
    "\n",
    "This transformation would typically be part of a job used to refresh our feature, triggered for model training and inference so that the features are computed with the same code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91f5c2d9-b89f-4ce3-9c47-53ba838067d3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create our features using Pandas API on top of spark"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Get our table and switch to pandas APIs\n",
    "df = spark.table('travel_purchase').pandas_api()\n",
    "\n",
    "#Add features from the time variable \n",
    "def add_time_features(df):\n",
    "    # Extract day of the week, day of the month, and hour from the ts column\n",
    "    df['day_of_week'] = df['ts'].dt.dayofweek\n",
    "    df['day_of_month'] = df['ts'].dt.day\n",
    "    df['hour'] = df['ts'].dt.hour\n",
    "    \n",
    "    # Calculate sin and cos values for the day of the week, day of the month, and hour\n",
    "    df['day_of_week_sin'] = np.sin(df['day_of_week'] * (2 * np.pi / 7))\n",
    "    df['day_of_week_cos'] = np.cos(df['day_of_week'] * (2 * np.pi / 7))\n",
    "    df['day_of_month_sin'] = np.sin(df['day_of_month'] * (2 * np.pi / 30))\n",
    "    df['day_of_month_cos'] = np.cos(df['day_of_month'] * (2 * np.pi / 30))\n",
    "    df['hour_sin'] = np.sin(df['hour'] * (2 * np.pi / 24))\n",
    "    df['hour_cos'] = np.cos(df['hour'] * (2 * np.pi / 24))\n",
    "    df = df.drop(['ts', 'day_of_week', 'day_of_month', 'hour'], axis=1)\n",
    "    return df\n",
    "\n",
    "df[\"clicked\"] = df[\"clicked\"].astype(int)\n",
    "df = add_time_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "867114c7-448e-4542-a318-5c3ef28ce347",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Labels shouldn't be part of the feature to avoid leaking result to our models"
    }
   },
   "outputs": [],
   "source": [
    "#Drop the label column from our dataframe\n",
    "df = df.drop(\"purchased\", axis=1)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94eb9079-08fd-4473-8a48-fef2eaf49001",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Save the Feature Engineering Table\n",
    "\n",
    "Next, we will save our feature as a Feature Engineering Table using the **`create_table`** method.\n",
    "\n",
    "We'll need to give it a name and a primary key that we'll use for lookup. Primary key should be unique. In this case we'll use the booking id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78bd421d-3d6a-4b10-a752-f921c85f5c8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Let's start creating a <a href=\"https://docs.databricks.com/en/machine-learning/feature-store/uc/feature-tables-uc.html#create-a-feature-table-in-unity-catalog&language-Python\" target=\"_blank\">Feature Engineering Client</a>. Calling `create_table` on this client will result in a table being created in Unity Catalog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0994c7b9-d009-4bc9-8dfc-0fe266245c3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient, FeatureLookup\n",
    "\n",
    "fe = FeatureEngineeringClient(model_registry_uri=\"databricks-uc\")\n",
    "\n",
    "fe.create_table(\n",
    "    name=\"destination_location_fs\",\n",
    "    primary_keys=[\"id\"],\n",
    "    df=df.to_spark(),\n",
    "    description=\"Travel purchases dataset with purchase timestamp\",\n",
    "    tags={\"team\":\"analytics\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2739712f-c8b1-4cab-884e-0cea0f94cc93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Alternatively, you can first **`create_table`** with a schema only, and populate data to the feature table with **`fs.write_table`**. To add data you can simply use **`fs.write_table`** again. **`fs.write_table`** supports a **`merge`** mode to update features based on the primary key. To overwrite a feature table you can simply `DELETE` the existing records directly from the feature table before writing new data to it, again with **`fs.write_table`**.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "fe.create_table(\n",
    "    name=\"destination_location_fs\",\n",
    "    primary_keys=[\"destination_id\"],\n",
    "    schema=destination_features_df.schema,\n",
    "    description=\"Destination Popularity Features\",\n",
    ")\n",
    "\n",
    "fe.write_table(\n",
    "    name=\"destination_location_fs\",\n",
    "    df=destination_features_df\n",
    ")\n",
    "\n",
    "# And then later/in the next run...\n",
    "fe.write_table(\n",
    "    name=\"destination_location_fs\",\n",
    "    df=updated_destination_features_df,\n",
    "    mode=\"merge\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d485554f-5ff1-4fb5-bfbc-d4ac01fadbd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Our table is now ready!\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/feature_store/feature-store-basic-fs-table.png?raw=true\" width=\"700px\" style=\"float: right\">\n",
    "\n",
    "We can explore the created feature engineering table using the Unity Catalog Explorer. \n",
    "\n",
    "From within the Explorer, select your catalog and browse the tables in the dropdown.\n",
    "\n",
    "We can view some sample data from the `travel_recommender_basic` table that was just created.\n",
    "\n",
    "Additionally, we can also use the `fe.get_table()` method to get metadata associated with our newly created table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9167114-cbe3-42c5-b8fb-34f374bfc054",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fe_get_table = fe.get_table(name=\"destination_location_fs\")\n",
    "print(f\"Feature Table in UC=destination_location_fs. Description: {fe_get_table.description}\")\n",
    "print(\"The table contains those features: \", fe_get_table.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f73f5627-69be-46fe-80a4-5eea15418d05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 2: Train a model with FS \n",
    "\n",
    "\n",
    "We'll now train a ML model using the feature stored in our datasets.\n",
    "\n",
    "* First we need to build or training dataset. We'll need to provide a list of destination id (used as our feature table primary key) and the associated label we want to predict. We'll then retrieve the features from the feature table using a Feature Lookup list which will join the data based on the lookup key **`id`**\n",
    "* We'll then train our model using these features\n",
    "* Finally, we'll deploy this model in production.\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/main/images/product/feature_store/feature_store_training.png\" style=\"margin-left: 10px\" width=\"1200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a689f89-f628-4586-9e9e-2becac9033bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Build the training dataset \n",
    "\n",
    "Let's start by building the dataset, retrieving features from our feature table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba105dbc-f325-42b7-b16d-b03444b3db98",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get our list of id & labels"
    }
   },
   "outputs": [],
   "source": [
    "id_and_label = spark.table('travel_purchase').select(\"id\", \"purchased\")\n",
    "display(id_and_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf88d154-f739-49e7-b89f-49f1c73a2f12",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Retrieve the features from the feature table"
    }
   },
   "outputs": [],
   "source": [
    "model_feature_lookups = [\n",
    "      FeatureLookup(\n",
    "          table_name=\"destination_location_fs\",\n",
    "          lookup_key=[\"id\"],\n",
    "          #feature_names=[\"...\"], # if you dont specify here the FS will take all your features apart from primary_keys \n",
    "      )\n",
    "]\n",
    "# fe.create_training_set will look up features in model_feature_lookups with matched key from training_labels_df\n",
    "training_set = fe.create_training_set(\n",
    "    df=id_and_label, # joining the original Dataset, with our FeatureLookupTable\n",
    "    feature_lookups=model_feature_lookups,\n",
    "    exclude_columns=[\"user_id\", \"id\", \"booking_date\"], # exclude features we won't use in our model\n",
    "    label='purchased',\n",
    ")\n",
    "\n",
    "training_pd = training_set.load_df().toPandas()\n",
    "display(training_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c4977e8-6dc7-4ef9-ac3c-c80fa20a7594",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Training our baseline model \n",
    "\n",
    "Note that for our first basic example, the feature used are very limited and our model will very likely not be efficient, but we won't focus on the model performance.\n",
    "\n",
    "The following steps will be a basic LGBM model. For a more complete ML training example including hyperparameter tuning, we recommend using Databricks Auto ML and exploring the generated notebooks.\n",
    "\n",
    "Note that to log the model, we'll use the `FeatureEngineeringClient.log_model(...)` function and not the usual `mlflow.skearn.log_model(...)`. This will capture all the feature dependencies & lineage for us and update the feature table data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f4493d3-92d2-4c5b-a4a6-838a11d03e61",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Split the dataset"
    }
   },
   "outputs": [],
   "source": [
    "X_train = training_pd.drop('purchased', axis=1)\n",
    "Y_train = training_pd['purchased'].values.ravel()\n",
    "x_train, x_val,  y_train, y_val = train_test_split(X_train, Y_train, test_size=0.10, stratify=Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3cad3f9-e661-47bd-8c7a-31c4089ee99e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Train a model using the training dataset & log it using the Feature Engineering client"
    }
   },
   "outputs": [],
   "source": [
    "mlflow.sklearn.autolog(log_input_examples=True,silent=True)\n",
    "model_name = \"dbdemos_fs_travel_model\"\n",
    "model_full_name = f\"{catalog}.{db}.{model_name}\"\n",
    "dataset = mlflow.data.from_pandas(X_train)\n",
    "\n",
    "with mlflow.start_run(run_name=\"lightGBM\") as run:\n",
    "  #Define our LGBM model\n",
    "  mlflow.log_input(dataset, \"training\")\n",
    "  numerical_pipeline = Pipeline(steps=[\n",
    "    (\"converter\", FunctionTransformer(lambda df: df.apply(pd.to_numeric, errors=\"coerce\"))),\n",
    "    (\"standardizer\", StandardScaler())])\n",
    "  one_hot_pipeline = Pipeline(steps=[(\"one_hot_encoder\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "  preprocessor = ColumnTransformer([(\"numerical\", numerical_pipeline, [\"clicked\", \"price\"]),\n",
    "                                    (\"onehot\", one_hot_pipeline, [\"clicked\", \"destination_id\"])], \n",
    "                                    remainder=\"passthrough\", sparse_threshold=0)\n",
    "  model = Pipeline([\n",
    "      (\"preprocessor\", preprocessor),\n",
    "      (\"classifier\", LGBMClassifier(**params)),\n",
    "  ])\n",
    "\n",
    "  #Train the model\n",
    "  model.fit(x_train, y_train)  \n",
    "\n",
    "  #log the model. Note that we're using the fs client to do that\n",
    "  fe.log_model(\n",
    "              model=model, # object of your model\n",
    "              artifact_path=\"model\", #name of the Artifact under MlFlow\n",
    "              flavor=mlflow.sklearn, # flavour of the model (our LightGBM model has a SkLearn Flavour)\n",
    "              training_set=training_set, # training set you used to train your model with AutoML\n",
    "              registered_model_name=model_full_name, # register your best model\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "635e90fc-834a-4cd6-b2fb-77e9e4b06342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " \n",
    "#### Our model is now saved in Unity Catalog. \n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/feature_store/feature-store-basic-model-uc.png?raw=true\" width=\"700px\" style=\"float: right\"/>\n",
    "\n",
    "You can open the right menu to see the newly created \"lightGBM\" experiment, containing the model.\n",
    "\n",
    "In addition, the model also appears in Catalog Explorer, under the catalog we created earlier. This way, our tables and models are logically grouped together under the same catalog, making it easy to see all assets, whether data or models, associated with a catalog.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9a8b967-318b-4017-bd80-47edb373afb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Table lineage\n",
    "\n",
    "Lineage is automatically captured and visible within Unity Catalog. It tracks all tables up to the model created.\n",
    "\n",
    "This makes it easy to track all your data usage, and downstream impact. If some PII information got leaked, or some incorrect data is loaded and detected by the Lakehouse Monitoring, it's then easy to track the potential impact.\n",
    "\n",
    "Note that this not only includes table and model, but also Notebooks, Dashboard, Jobs triggering the run etc.\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/feature_store/feature-store-basic-fs-table-lineage.png?raw=true\" width=\"700px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab11a4f8-8968-4422-a0cb-ab4f162681b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Move the model to Production\n",
    "\n",
    "Because we used the `registered_model_name` parameter, our model was automatically added to the registry. \n",
    "\n",
    "We can now chose to move it in Production. \n",
    "\n",
    "*Note that a typical ML pipeline would first run some tests & validation before doing moving the model as Production. We'll skip this step to focus on the Feature Engineering capabilities*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d1f8261-58d4-4065-a4c6-19547603b900",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Move the last version in production"
    }
   },
   "outputs": [],
   "source": [
    "mlflow_client = MlflowClient()\n",
    "# Use the MlflowClient to get a list of all versions for the registered model in Unity Catalog\n",
    "all_versions = mlflow_client.search_model_versions(f\"name='{model_full_name}'\")\n",
    "# Sort the list of versions by version number and get the latest version\n",
    "latest_version = max([int(v.version) for v in all_versions])\n",
    "# Use the MlflowClient to get the latest version of the registered model in Unity Catalog\n",
    "latest_model = mlflow_client.get_model_version(model_full_name, str(latest_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff666766-4804-47f4-95a0-feda189a6bd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Move it in Production\n",
    "production_alias = \"production\"\n",
    "if len(latest_model.aliases) == 0 or latest_model.aliases[0] != production_alias:\n",
    "  print(f\"updating model {latest_model.version} to Production\")\n",
    "  mlflow_client.set_registered_model_alias(model_full_name, production_alias, version=latest_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76ff2ea7-ce97-4e9c-b4af-ffd3b381d6d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3: Running inferences\n",
    "\n",
    "We are now ready to run inferences.\n",
    "\n",
    "In a real world setup, we would receive new data from our customers and have our job incrementally refreshing our customer features running in parallel. \n",
    "\n",
    "To make the predictions, all we need to have is our customer ID. Feature Engineering in UC will automatically do the lookup for us as defined in the training steps.\n",
    "\n",
    "This is one of the great outcome using the Feature Engineering in UC: you know that your features will be used the same way for inference as training because it's being saved with your feature store metadata.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/main/images/product/feature_store/feature_store_inference.png\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3309633a-9ee0-434c-9b30-be4851f97608",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run inferences from a list of IDs"
    }
   },
   "outputs": [],
   "source": [
    "# Load the ids we want to forecast\n",
    "## For sake of simplicity, we will just predict using the same ids as during training, but this could be a different pipeline\n",
    "id_to_forecast = spark.table('travel_purchase').select(\"id\")\n",
    "\n",
    "scored_df = fe.score_batch(model_uri=f\"models:/{model_full_name}@{production_alias}\", df=id_to_forecast, result_type=\"boolean\")\n",
    "display(scored_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "391b28a8-8f37-4ea0-877d-026add61a973",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Note that while we only selected a list of ID, we get back as result our prediction (is this user likely to book this travel `True`/`False`) and the full list of features automatically retrieved from our feature table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4144ffd-34b8-4c6f-aec8-d46f721ab6ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Summary \n",
    "\n",
    "We've seen a first basic example, creating a Feature Engineering table and training a model on top of that.\n",
    "\n",
    "Databricks Feature Engineering in Unity Catalog brings you a full traceability, knowing which model is using which feature in which notebook/job.\n",
    "\n",
    "It also simplify inferences by always making sure the same features will be used for model training and inference, always querying the same feature table based on your lookup keys.\n",
    "\n",
    "\n",
    "## Next Steps \n",
    "\n",
    "We'll go more in details and introduce more feature engineering capabilities in the next demos:\n",
    "\n",
    "\n",
    "Open the [02_Feature_store_advanced notebook]($./02_Feature_store_advanced) to explore more Feature Engineering in Unity Catalog benefits & capabilities:\n",
    "- Multiple lookup tables\n",
    "- Leveraging Databricks Automl to get a more advanced model\n",
    "- Using point in time lookups\n",
    "- Deploy online tabels for realtime model serving"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_Feature_store_introduction",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
